## A camada de abstração do banco de dados (DAL)
``DAL``:inxx

### Dependências

O web2py vem com uma Camada de Abstração de Banco de Dados (DAL), uma API que mapeia objetos Python em objetos de banco de dados, como consultas, tabelas e registros. O DAL gera dinamicamente o SQL em tempo real usando o dialeto especificado para o back end do banco de dados, para que você não precise escrever código SQL ou aprender diferentes dialetos SQL (o termo SQL é usado genericamente), e o aplicativo será portável entre diferentes tipos de bases de dados. Uma lista parcial de bancos de dados suportados é mostrada na tabela abaixo. Por favor, verifique no web2py site e lista de discussão para adaptadores mais recentes. O Google NoSQL é tratado como um caso particular no Capítulo 13.

A seção Gotchas no final deste capítulo contém mais algumas informações sobre bancos de dados específicos.

A distribuição binária do Windows funciona fora da caixa com SQLite, MSSQL, Postgresql e MySQL. A distribuição binária do Mac funciona fora da caixa com o SQLite.
Para usar qualquer outro back-end de banco de dados, execute a partir da distribuição de origem e instale o driver apropriado para o back end necessário.
``database drivers``:inxx

Uma vez instalado o driver correto, inicie o web2py a partir da fonte e ele encontrará o driver. Aqui está uma lista dos drivers que o web2py pode usar:

``DAL``:inxx ``SQLite``:inxx ``MySQL``:inxx ``PostgresSQL``:inxx ``Oracle``:inxx ``MSSQL``:inxx ``FireBird``:inxx ``DB2``:inxx ``Informix``:inxx ``Sybase``:inxx ``Teradata``:inxx ``MongoDB``:inxx ``CouchDB``:inxx ``SAPDB``:inxx ``Cubrid``:inxx

----------
database | drivers (source)
SQLite | sqlite3 ou pysqlite2 ou zxJDBC ``zxjdbc``:cite  (on Jython)
PostgreSQL | psycopg2 ``psycopg2``:cite ou zxJDBC ``zxjdbc``:cite  (on Jython)
MySQL | pymysql ``pymysql``:cite ou MySQLdb ``mysqldb``:cite
Oracle | cx_Oracle ``cxoracle``:cite
MSSQL | pyodbc ``pyodbc``:cite ou pypyodbc``pypyodbc``:cite
FireBird | kinterbasdb ``kinterbasdb``:cite ou fdb ou pyodbc
DB2 | pyodbc ``pyodbc``:cite
Informix | informixdb ``informixdb``:cite
Ingres | ingresdbi ``ingresdbi``:cite
Cubrid | cubriddb ``cubridb``:cite ``cubridb``:cite
Sybase | Sybase ``Sybase``:cite
Teradata | pyodbc ``Teradata``:cite
SAPDB    | sapdb ``SAPDB``:cite
MongoDB | pymongo ``pymongo``:cite
IMAP | imaplib ``IMAP``:cite
---------

``sqlite3``, ``pymysql``, e ``imaplib`` acompanham a versão padrão do web2py. O suporte do MongoDB é experimental. A opção IMAP permite usar o DAL para acessar o IMAP.

### O DAL: Um tour rápido
web2py define as seguintes classes que compõem o DAL:

O objeto **DAL** representa uma conexão com o banco de dados. Por exemplo:
``sqlite``:inxx
``
db = DAL('sqlite://storage.sqlite')
``:code

``define_table``:inxx
**Table** representa uma tabela de banco de dados. Você não instancia diretamente Table; em vez disso, ``DAL.define_table`` a instancia.
``
db.define_table('mytable', Field('myfield'))
``:code

Os métodos mais importantes de uma tabela são:
``insert``:inxx
``truncate``:inxx
``drop``:inxx
``import_from_csv_file``:inxx
``count``:inxx
``.insert`` (inserir),

``.truncate`` (limpar todos os dados da tabela),

``.drop`` (excluir registro(s) ), e

``.import_from_csv_file`` (importar de um arquivo csv).

``Field``:inxx
**Field** representa um campo de banco de dados. Pode ser instanciado e passado como um argumento para ``DAL.define_table``.

``Rows``:inxx
**DAL Rows** ``Row``:inxx  é o objeto retornado por um 'select' (consulta) ao  banco de dados. Pode ser pensado como uma lista de linhas ``Row``:
``
rows = db(db.mytable.myfield != None).select()
``:code

``Row``:inxx
**Row** contém valores de campo.
``
for row in rows:
    print row.myfield
``:code

``Query``:inxx
**Query** é um objeto que representa uma cláusula SQL "where" (onde):
``
myquery = (db.mytable.myfield != None) | (db.mytable.myfield > 'A')
``:code

``Set``:inxx
**Set** é um objeto que representa um conjunto de registros.
Seus métodos mais importantes são ``count`` (contar), ``select`` (selecionar),
``update`` (atualizar), e ``delete`` (excluir). Por exemplo:
``
myset = db(myquery)
rows = myset.select()
myset.update(myfield='somevalue')
myset.delete()
``:code

``Expression``:inxx

**Expression** é algo como uma expressão ``orderby`` e ``group by``. A classe Field é derivada da expressão. Aqui está um exemplo.
``
myorder = db.mytable.myfield.upper() | db.mytable.id
db().select(db.table.ALL, orderby=myorder)
``:code

### Usando o DAL "autônomo"
O web2py DAL pode ser usado em um ambiente que não seja web2py via
``
from gluon import DAL, Field
# also consider: from gluon.validators import *
``:code

[[dal_constructor]]
### Construtor DAL
Uso básico:
``
>>> db = DAL('sqlite://storage.sqlite')
``:code

O banco de dados agora está conectado e a conexão é armazenada na variável global `` db``.

A qualquer momento, você pode recuperar a string de conexão
``_uri``:inxx
``
>>> print db._uri
sqlite://storage.sqlite
``:code

e o nome do banco de dados
``_dbname``:inxx
``
>>> print db._dbname
sqlite
``:code

A string de conexão é chamada `` _uri`` porque é uma instância de um Uniform Resource Identifier (Identificador Uniforme de Recursos).

O DAL permite múltiplas conexões com o mesmo banco de dados ou com diferentes bancos de dados, até bancos de dados de diferentes tipos. Por enquanto, vamos assumir a presença de um único banco de dados, já que esta é a situação mais comum.
#### Assinatura DAL
``
DAL(uri='sqlite://dummy.db',
    pool_size=0,
    folder=None,
    db_codec='UTF-8',
    check_reserved=None,
    migrate=True,
    fake_migrate=False,
    migrate_enabled=True,
    fake_migrate_all=False,
    decode_credentials=False,
    driver_args=None,
    adapter_args=None,
    attempts=5,
    auto_import=False,
    bigint_id=False,
    debug=False,
    lazy_tables=False,
    db_uid=None,
    do_connect=True,
    after_connection=None,
    tables=None,
    ignore_field_case=True,
    entity_quoting=False,
    table_hash=None)
``:code

[[connection_strings]]
#### Strings de conexão (o parâmetro uri)
``connection strings``:inxx

Uma conexão com o banco de dados é estabelecida criando uma instância do objeto DAL:
``
>>> db = DAL('sqlite://storage.sqlite', pool_size=0)
``:code
``db`` não é uma palavra-chave; é uma variável local que armazena o objeto de conexão `` DAL``. Você é livre para dar um nome diferente. O construtor de `` DAL`` requer um único argumento, a string de conexão. A cadeia de conexão é o único código web2py que depende de um banco de dados back-end específico. Aqui estão exemplos de strings de conexão para tipos específicos de bancos de dados back-end suportados (em todos os casos, assumimos que o banco de dados está sendo executado a partir do localhost em sua porta padrão e é chamado de "test"):

``ndb``:index

-------------
**SQLite**     | ``sqlite://storage.sqlite``
**MySQL**      | ``mysql://username:password@localhost/test``
**PostgreSQL** | ``postgres://username:password@localhost/test``
**MSSQL (legacy)**      | ``mssql://username:password@localhost/test``
**MSSQL (>=2005)**      | ``mssql3://username:password@localhost/test``
**MSSQL (>=2012)**      | ``mssql4://username:password@localhost/test``
**FireBird**   | ``firebird://username:password@localhost/test``
**Oracle**     | ``oracle://username/password@test``
**DB2**        | ``db2://username:password@test``
**Ingres**     | ``ingres://username:password@localhost/test``
**Sybase**     | ``sybase://username:password@localhost/test``
**Informix**   | ``informix://username:password@test``
**Teradata**   | ``teradata://DSN=dsn;UID=user;PWD=pass;DATABASE=test``
**Cubrid**     | ``cubrid://username:password@localhost/test``
**SAPDB**      | ``sapdb://username:password@localhost/test``
**IMAP**       | ``imap://user:password@server:port``
**MongoDB**    | ``mongodb://username:password@localhost/test``
**Google/SQL** | ``google:sql://project:instance/database``
**Google/NoSQL** | ``google:datastore``
**Google/NoSQL/NDB** | ``google:datastore+ndb``
-------------

Observe que no SQLite o banco de dados consiste em um único arquivo. Se não existir, é criado. Este arquivo é bloqueado toda vez que é acessado. No caso do MySQL, PostgreSQL, MSSQL, FireBird, Oracle, DB2, Ingres e Informix, o banco de dados "teste" deve ser criado fora do web2py. Uma vez estabelecida a conexão, o web2py criará, alterará e excluirá as tabelas apropriadamente.

No caso do Google/NoSQL, a opção `` + ndb`` ativa o NDB. O NDB usa um buffer do Memcache para ler dados que são acessados com freqüência. Isso é completamente automático e feito no nível do armazenamento de dados, não no nível web2py.

Também é possível definir a string de conexão para ``None``. Nesse caso, o DAL não se conectará a nenhum banco de dados de back-end, mas a API ainda poderá ser acessada para testes. Exemplos disso serão discutidos no Capítulo 7.

Algumas vezes, você pode precisar gerar o SQL como se tivesse uma conexão, mas sem realmente se conectar ao banco de dados. Isso pode ser feito com
``
db = DAL('...', do_connect=False)
``:code


Neste caso, você poderá chamar `` _select``, `` _insert``, `` _update`` e `` _delete`` para gerar SQL, mas não chamar `` select``, `` insert`` , `` update`` e `` delete``. Na maioria dos casos você pode usar `` do_connect = False`` mesmo sem ter os drivers de banco de dados necessários.

Observe que, por padrão, o web2py usa a codificação de caracteres utf8 para bancos de dados. Se você trabalha com bancos de dados existentes que se comportam de maneira diferente, você deve alterá-lo com o parâmetro opcional `` db_codec`` como

``
db = DAL('...', db_codec='latin1')
``:code

Caso contrário, você obterá tickets UnicodeDecodeError.


#### Pool de conexão (um conjunto de conexões)
``connection pooling``:inxx
Um argumento comum do construtor DAL é o `` pool_size``; o padrão é zero.

Como é lento estabelecer uma nova conexão de banco de dados para cada solicitação, o web2py implementa um mecanismo para o pool de conexões. Quando uma conexão é estabelecida e a página é veiculada e a transação é concluída, a conexão não é fechada, mas entra em um pool. Quando a próxima solicitação http chegar, o web2py tentará reciclar uma conexão do pool e usá-la para a nova transação. Se não houver conexões disponíveis no pool, uma nova conexão será estabelecida.

Quando o web2py é iniciado, o pool está sempre vazio. O conjunto cresce até o mínimo entre o valor de `` pool_size`` e o número máximo de solicitações simultâneas. Isto significa que, se `` pool_size = 10`` mas nosso servidor nunca receber mais de 5 solicitações simultâneas, o tamanho real do conjunto crescerá para 5. Se `` pool_size = 0``, o conjunto de conexões não será usado.

As conexões nos conjuntos são compartilhadas seqüencialmente entre os encadeamentos, no sentido de que podem ser usadas por dois encadeamentos diferentes, mas não simultâneos. Existe apenas um pool para cada processo web2py.

O parâmetro `` pool_size`` é ignorado pelo SQLite e pelo Google App Engine.
O pool de conexões é ignorado pelo SQLite, pois não produziria nenhum benefício.

#### Falhas de conexão (parâmetro de tentativas)

Se o web2py não conseguir se conectar ao banco de dados, ele aguardará 1 segundo e, por padrão, tentará novamente até 5 vezes antes de declarar uma falha. No caso de pool de conexão, é possível que uma conexão em pool que permaneça aberta, mas não usada por algum tempo, seja fechada pelo final do banco de dados. Graças ao recurso de repetição, o web2py tenta restabelecer essas conexões descartadas.
O número de tentativas é definido por meio do parâmetro de tentativas. ``attempts=5``

#### Lazy Tables (Tabelas Preguiçosas)
definindo `` lazy_tables = True`` fornece um grande aumento de desempenho. Veja abaixo: [[tabelas preguiçosas #lazy_tables]]

#### Aplicações sem modelo
Usar o diretório de modelos do web2py para seus modelos de aplicativos é muito conveniente e produtivo. Com tabelas preguiçosas e modelos condicionais, o desempenho é geralmente aceitável mesmo para grandes aplicações. Muitos desenvolvedores experientes usam esses ambientes de produção.

No entanto, é possível definir tabelas DAL sob demanda dentro das funções ou módulos do controlador. Isso pode fazer sentido quando o número ou a complexidade das definições de tabela sobrecarregar o uso de tabelas preguiçosas e modelos condicionais.

Isso é chamado de desenvolvimento "sem modelo" pela comunidade web2py.
 Isso significa menos uso da execução automática de arquivos python no diretório do modelo.
Isso não implica abandonar o conceito de modelos, visualizações e controladores.

A execução automática do código python pelo Web2py dentro do diretório do modelo faz isso por você:

+ os modelos são executados automaticamente sempre que uma solicitação é processada
+ modelos acessam o escopo global do web2py.

Os modelos também oferecem sessões de shell interativas úteis quando o web2py é iniciado com a opção de linha de comando -M.

Além disso, lembre-se de manutenção: outros desenvolvedores web2py esperam encontrar definições de modelo no diretório do modelo.

Para usar a abordagem "sem modelo", você assume a responsabilidade por executar essas duas tarefas domésticas.
Você chama as definições de tabela quando precisa delas e fornece o acesso necessário ao escopo global por meio do objeto atual. (conforme documentado no capítulo 4 [[compartilhamento global com o objeto atual ../04#current_object]])

Por exemplo, um aplicativo sem modelo típico pode deixar as definições dos objetos de conexão do banco de dados no arquivo de modelo, mas definir as tabelas sob demanda por função do controlador.

O caso típico é mover as definições da tabela para um arquivo de módulo (um arquivo python salvo no diretório modules).

Se a função para definir um conjunto de tabelas é chamada de `` define_employee_tables () `` em um módulo chamado "table_setup.py", o controlador que deseja referir-se às tabelas relacionadas a registros de funcionários para fazer com que um SQLFORM precise chamar a função `` define_employee_tables () `` antes de acessar qualquer tabela. A função `` define_employee_tables () `` precisa acessar o objeto de conexão com o banco de dados para definir as tabelas. É por isso que você precisa usar corretamente o objeto `` current`` no arquivo do módulo contendo `` define_employee_tables () `` (como mencionado acima).

#### Bancos de Dados Replicados

O primeiro argumento do `` DAL (...) `` pode ser uma lista de URIs. Neste caso, o web2py tenta se conectar a cada um deles. O principal objetivo para isso é lidar com vários servidores de banco de dados e distribuir a carga de trabalho entre eles). Aqui está um caso de uso típico:

``
db = DAL(['mysql://...1', 'mysql://...2', 'mysql://...3'])
``:code

Neste caso, o DAL tenta se conectar ao primeiro e, em caso de falha,
vai tentar o segundo e o terceiro. Isso também pode ser usado para distribuir carga
em uma configuração mestre-escravo do banco de dados. Nós falaremos mais sobre isso
no Capítulo 13, no contexto da escalabilidade.

#### Palavras-chave reservadas
``reserved Keywords``:inxx

``check_reserved`` informa ao construtor para verificar nomes de tabelas e nomes de colunas em relação a palavras-chave SQL reservadas em bancos de dados de backend de destino. O ``check_reserved`` é padronizado para ``None``.

Esta é uma lista de sequências que contêm os nomes do adaptador de backend do banco de dados.

O nome do adaptador é o mesmo usado na cadeia de conexão do DAL. Portanto, se você quiser verificar o PostgreSQL e o MSSQL, sua string de conexão será a seguinte:
``
db = DAL('sqlite://storage.sqlite',
         check_reserved=['postgres', 'mssql'])
``:code

The DAL will scan the keywords in the same order as of the list.

Existem duas opções extras "all" e "common". Se você especificar all, ele verificará todas as palavras-chave SQL conhecidas. Se você especificar common, ele verificará somente as palavras-chave SQL comuns, como `` SELECT``, `` INSERT``, `` UPDATE``, etc.

Para back-ends suportados, você também pode especificar se deseja verificar também as palavras-chave SQL não reservadas. Neste caso, você acrescentaria `` _nonreserved`` ao nome. Por exemplo:
``
check_reserved=['postgres', 'postgres_nonreserved']
``:code

Os seguintes backends de banco de dados suportam a verificação de palavras reservadas.

-----
**PostgreSQL** | ``postgres(_nonreserved)``
**MySQL** | ``mysql``
**FireBird** | ``firebird(_nonreserved)``
**MSSQL** | ``mssql``
**Oracle** | ``oracle``
-----

#### Citação de banco de dados e configurações de caso (entity_quoting, ignore_field)

Você também pode usar a referência explícita de entidades SQL no nível DAL. Ele funciona de forma transparente para que você possa usar os mesmos nomes em python e no esquema do banco de dados.

``ignore_field_case = True``
``entity_quoting = True``
Aqui está um exemplo:
``
db = DAL('postgres://...', ..., ignore_field_case=False, entity_quoting=True)

db.define_table('table1', Field('column'), Field('COLUMN'))

print db(db.table1.COLUMN != db.table1.column).select()
``:code

#### Fazendo uma conexão segura

Às vezes é necessário (e aconselhado) conectar-se ao seu banco de dados usando conexão segura, especialmente se o seu banco de dados não estiver no mesmo servidor que o seu aplicativo. Nesse caso, você precisa passar parâmetros adicionais para o driver do banco de dados. Você deve consultar a documentação do driver do banco de dados para obter detalhes.

Para o Postgresql com o psycopg2, deve ficar assim:

``
DAL('postgres://user_name:user_password@server_addr/db_name', driver_args={"sslmode":"require", "sslrootcert":"root.crt", "sslcert":"postgresql.crt", "sslkey":"postgresql.key"})
``:code

onde os parâmetros ``sslrootcert``, ``sslcert`` e ``sslkey`` devem conter o caminho completo para os arquivos. Você deve consultar a documentação do Postgresql sobre como configurar o servidor Postgresql para aceitar conexões seguras.

#### Outros parâmetros do construtor DAL

##### Localização da pasta de banco de dados
`` folder`` - onde os arquivos .table serão criados. Definir automaticamente no web2py. Use um caminho explícito ao usar o DAL fora do web2py

##### Configurações de migração padrão
A migração está detalhada abaixo em Tabelas [[table migrations #table_migrations]]. As configurações de migração do construtor DAL são booleanas que afetam os padrões e o comportamento global.

`` migrate = True`` define o comportamento padrão de migração para todas as tabelas

`` fake_migrate = False`` define o comportamento padrão do fake_migrate para todas as tabelas

`` migrate_enabled = True`` Se definido como False, desativa TODAS as migrações

`` fake_migrate_all = False`` Se definido como True fake, migra TODAS as tabelas

#### Experimente com o shell web2py

Você pode experimentar a API DAL usando o shell web2py (-S [[opção de linha de comando ../04#CommandLineOptions]]).

Comece criando uma conexão. Por exemplo, você pode usar o SQLite. Nada nesta discussão muda quando você altera o mecanismo de back-end.

[[table_constructor]]
### Construtor de tabela
``define_table``:inxx ``Field``:inxx

#### assinatura de define_table
A assinatura de define_table:

Tabelas são definidas no DAL via `` define_table``:
``
>>> db.define_table('person',
                    Field('name'),
                    id=id,
                    rname=None,
                    redefine=True
                    common_filter,
                    fake_migrate,
                    fields,
                    format,
                    migrate,
                    on_define,
                    plural,
                    polymodel,
                    primarykey,
                    redefine,
                    sequence_name,
                    singular,
                    table_class,
                    trigger_name)
``:code

Ele define, armazena e retorna um objeto `` Table`` chamado "person" contendo um campo (coluna) "name". Este objeto também pode ser acessado via `` db.person``, então você não precisa pegar o valor de retorno.

#### ``id``: Notas sobre a chave primária

Não declare um campo chamado "id", porque um é criado pelo web2py mesmo assim. Cada tabela tem um campo chamado "id" por padrão. É um campo inteiro de auto-incremento (começando em 1) usado para referência cruzada e para tornar cada registro único, então "id" é uma chave primária. (Observação: o contador de ID começando em 1 é específico de back-end. Por exemplo, isso não se aplica ao NoSQL do Google App Engine.)

``named id field``:inxx

Opcionalmente, você pode definir um campo de `` type = 'id'`` e o web2py usará este campo como campo de ID de incremento automático. Isso não é recomendado, exceto ao acessar tabelas de banco de dados herdadas que possuem uma chave primária com um nome diferente.
Com alguma limitação, você também pode usar chaves primárias diferentes usando o parâmetro `` primarykey``. [[primarykey #primarykey]] é explicado logo abaixo.

#### ``plural`` and ``singular``
Objetos Smartgrid podem precisar saber o nome singular e plural da tabela. Os padrões são inteligentes, mas esses parâmetros permitem que você seja específico. Veja smartgrid para mais informações.

#### ``redefine``
As tabelas podem ser definidas apenas uma vez, mas você pode forçar o web2py a redefinir uma tabela existente:

``
db.define_table('person', Field('name'))
db.define_table('person', Field('name'), redefine=True)
``:code

A redefinição pode acionar uma migração se o conteúdo do campo for diferente.

[[record_representation]]
#### ``format``: Registro de representação

É opcional, mas é recomendado especificar uma representação de formato para registros com o parâmetro `` format``.
``
>>> db.define_table('person', Field('name'), format='%(name)s')
``:code

ou
``
>>> db.define_table('person', Field('name'), format='%(name)s %(id)s')
``:code

ou ainda mais complexos usando uma função:
``
>>> db.define_table('person',
                    Field('name'),
                    format=lambda r: r.name or 'anonymous')
``:code

O atributo de formato será usado para dois propósitos:
- Para representar registros referenciados em drop-downs de seleção / opção.
- Para definir o atributo `` db.othertable.person.represent`` para todos os campos que fazem referência a essa tabela. Isso significa que o SQLTABLE não mostrará referências por id, mas usará a representação preferencial de formato.

#### ``rname``: Registro de representação
`` rname`` define um nome de backend do banco de dados para a tabela. Isso torna o nome da tabela web2py um alias, e `` rname`` é o nome real usado ao construir a consulta para o backend.
Para ilustrar apenas um uso, `` rname`` pode ser usado para fornecer nomes de tabelas MSSQL totalmente qualificados acessando tabelas pertencentes a outros databases on the server: ``rname = 'db1.dbo.table1'``:code

[[primarykey]]
#### ``primarykey``: Suporte para tabelas legadas
O `` primarykey`` ajuda a suportar tabelas legadas com chaves primárias existentes, mesmo com múltiplas partes.
Veja [[Legacy Databases #LegacyDatabases]] abaixo.

#### ``migrate``, ``fake_migrate``
``migrate`` define opções de migração para a tabela. Veja [[Table Migrations #table_migrations]] abaixo

#### ``table_class``
Se você definir sua própria classe Table como uma subclasse de gluon.dal.Table, você pode fornecê-la aqui; isso permite estender e substituir métodos. Exemplo:``table_class=MyTable``:code

#### ``sequence_name``
(Opcional) O nome de uma sequência de tabela customizada (se suportada pelo banco de dados). Pode criar uma SEQUÊNCIA (começando em 1 e incrementando por 1) ou usar isto para tabelas legadas com seqüências customizadas. Observe que, quando necessário, o web2py criará sequências automaticamente por padrão (começando em 1).

#### ``trigger_name``
(opcional) Relaciona-se a `` sequence_name``. Relevante para alguns back-ends que não suportam campos numéricos de incremento automático.

#### ``polymodel``
Para Google App Engine

#### ``on_define``
``on_define`` é um retorno de chamada acionado quando um lazy_table é instanciado, embora seja chamado de qualquer maneira se a tabela não for preguiçosa. Isso permite alterações dinâmicas na tabela sem perder as vantagens da instanciação atrasada.

Exemplo:
``
db = DAL(lazy_tables=True)
db.define_table('person',
                Field('name'),
                Field('age', 'integer'),
                on_define=lambda table: [
                    table.name.set_attributes(requires=IS_NOT_EMPTY(), default=''),
                    table.age.set_attributes(requires=IS_INT_IN_RANGE(0, 120), default=30)
                    ]
                )
``:code
Note que este exemplo mostra como usar `` on_define``, mas não é realmente necessário. Os valores simples de `` requires`` poderiam ser adicionados às definições de Field e a tabela ainda seria preguiçosa. No entanto, `` requires`` que tomar um objeto definido como o primeiro argumento, como IS_IN_DB, vai fazer uma consulta como `` db.sometable.somefield == some_value``: código que causaria `` sometable`` ser definido cedo. Esta é a situação salva por `` on_define``.
[[lazy_tables]]
#### Lazy Tables, um grande aumento de desempenho
``lazy tables``:inxx
Os modelos web2py são executados antes dos controladores, portanto, todas as tabelas são definidas em todas as solicitações. Nem todas as tabelas são necessárias para lidar com cada solicitação, portanto, é possível que algum tempo gasto na definição de tabelas seja desperdiçado. Modelos condicionais ([[modelos condicionais, capítulo 4 ../04#conditional_models]]) podem ajudar, mas o web2py oferece um grande aumento de desempenho via lazy_tables. Esse recurso significa que a criação da tabela é adiada até que a tabela seja realmente referenciada. A ativação de tabelas lazy é feita ao inicializar um banco de dados por meio do construtor DAL. Isto requer a configuração do parâmetro `` DAL (..., lazy_tables = True) ``. Este é um dos impulsos de desempenho de tempo de resposta mais significativos no web2py.

#### Adicionando atributos a campos e tabelas
Se você precisar adicionar atributos personalizados aos campos, basta fazer isso:
``db.table.field.extra = {}``:code 

"extra" não é uma palavra-chave; Agora, são atributos personalizados anexados ao objeto de campo. Você pode fazê-lo com tabelas também, mas elas devem ser precedidas por um
sublinhado para evitar conflitos de nomenclatura com campos:

``db.table._extra = {} ``:code


[[field_constructor]]
### Construtor de campo
``Field constructor``:inxx
Estes são os valores padrão de um construtor de campo:
``
Field(fieldname, type='string', length=None, default=None,
      required=False, requires='<default>',
      ondelete='CASCADE', notnull=False, unique=False,
      uploadfield=True, widget=None, label=None, comment=None,
      writable=True, readable=True, update=None, authorize=None,
      autodelete=False, represent=None, compute=None,
      uploadfolder=None, uploadseparate=None, uploadfs=None, rname=None)
``:code

Nem todos eles são relevantes para todos os campos. "length" é relevante apenas para campos do tipo "string". "uploadfield" e "authorize" são relevantes apenas para campos do tipo "upload". "ondelete" é relevante apenas para os campos do tipo "reference" e "upload".
- `` length`` define o comprimento máximo de um campo "string", "password" ou "upload". Se `` length`` não for especificado, um valor padrão é usado, mas o valor padrão não é garantido para ser compatível com versões anteriores. "Para evitar migrações indesejadas nas atualizações, recomendamos que você sempre especifique o tamanho dos campos de string, senha e upload".
- `` default`` define o valor padrão para o campo. O valor padrão é usado ao executar uma inserção se um valor não for especificado explicitamente. Ele também é usado para preencher previamente formulários criados a partir da tabela usando o SQLFORM. Note, em vez de ser um valor fixo, o padrão pode ser uma função (incluindo uma função lambda) que retorna um valor do tipo apropriado para o campo. Nesse caso, a função é chamada uma vez para cada registro inserido, mesmo quando vários registros são inseridos em uma única transação.
- `` required`` informa ao DAL que nenhuma inserção deve ser permitida nesta tabela se um valor para este campo não for explicitamente especificado.
- `` requires`` é um validador ou uma lista de validadores. Isso não é usado pelo DAL, mas é usado pelo SQLFORM. Os validadores padrão para os tipos fornecidos são mostrados na próxima seção.
-------
Observe que `` requires = ... `` é aplicado no nível dos formulários, `` required = True`` é aplicado no nível do DAL (insert), enquanto `` notnull``, `` unique`` e `` ondelete`` são aplicados no nível do banco de dados. Embora às vezes possam parecer redundantes, é importante manter a distinção ao programar com o DAL.
-------
- `` uploadfolder`` enquanto o padrão é `` None``, a maioria dos adaptadores de banco de dados usará como padrão o upload de arquivos em os.path.join (request.folder, 'uploads'). O MongoAdapter não parece estar fazendo isso no momento.
- `` rname`` provê que o campo era um "nome real", um nome para o campo conhecido pelo adaptador de banco de dados; quando o campo é usado, é o valor do rname que é enviado para o banco de dados. O nome do web2py para o campo é efetivamente um alias.
`` ondelete``: inxx
- `` ondelete`` se traduz na instrução SQL "ON DELETE". Por padrão, é definido como "CASCADE". Isso informa ao banco de dados que, ao excluir um registro, ele também deve excluir todos os registros que se referem a ele. Para desabilitar este recurso, ajuste `` ondelete`` para "NO ACTION" ou "SET NULL".
- `` notnull = True`` se traduz na instrução SQL "NOT NULL". Isso evita que o banco de dados insira valores nulos para o campo.
- `` unique = True`` se traduz na declaração SQL "UNIQUE" e garante que os valores deste campo sejam únicos dentro da tabela. É aplicado no nível do banco de dados.
- `` uploadfield`` aplica-se apenas aos campos do tipo "upload". Um campo do tipo "upload" armazena o nome de um arquivo salvo em outro lugar, por padrão no sistema de arquivos sob a pasta "uploads /" do aplicativo. Se `` uploadfield`` estiver configurado como True, o arquivo será armazenado em um campo blob dentro da mesma tabela e o valor de `` uploadfield`` será o nome do campo blob. Isso será discutido em mais detalhes posteriormente no contexto do SQLFORM.
- O `` uploadfolder`` é padronizado para a pasta "uploads /" do aplicativo. Se definido para um caminho diferente, os arquivos serão enviados para uma pasta diferente.
-------
Por exemplo,
``
Field(..., uploadfolder=os.path.join(request.folder, 'static/temp'))
``:code
fará upload de arquivos para a pasta "web2py/applications/myapp/static/temp" .
-------
-`` uploadseparate`` se definido como True carregará arquivos em subpastas diferentes da pasta '' uploadfolder ''. Isso é otimizado para evitar muitos arquivos na mesma pasta / subpasta. ATENÇÃO: Você não pode alterar o valor de `` uploadseparate`` de True para False sem quebrar os links para os uploads existentes. O web2py usa as subpastas separadas ou não. A alteração do comportamento após o upload dos arquivos impedirá que o web2py recupere esses arquivos. Se isso acontecer, é possível mover arquivos e corrigir o problema, mas isso não é descrito aqui.
- `` uploadfs`` permite que você especifique um sistema de arquivos diferente para onde enviar arquivos, incluindo um armazenamento do Amazon S3 ou um armazenamento SFTP remoto. Esta opção requer o PyFileSystem instalado. ``uploadfs`` deve apontar para ``PyFileSystem``. ``PyFileSystem``:inxx ``uploadfs``:idxx
- `` widget`` deve ser um dos objetos de widget disponíveis, incluindo widgets customizados, por exemplo: ``SQLFORM.widgets.string.widget``. Uma lista de widgets disponíveis será discutida posteriormente. Cada tipo de campo tem um widget padrão.
- `` label`` é uma string (ou um auxiliar ou algo que pode ser serializado para uma string) que contém o rótulo a ser usado para este campo em formulários gerados automaticamente.
- `` comment`` é uma string (ou um auxiliar ou algo que pode ser serializado para uma string) que contém um comentário associado a este campo, e será exibido à direita do campo de entrada nos formulários auto-gerados.
- `` writable`` declara se um campo é gravável em formulários.
- `` readable`` declara se um campo é legível em formulários. Se um campo não for legível nem gravável, ele não será exibido nos formulários de criação e atualização.
- `` update`` contém o valor padrão para este campo quando o registro é atualizado.
- `` compute`` é uma função opcional. Se um registro for inserido ou atualizado, a função de cálculo será executada e o campo será preenchido com o resultado da função. O registro é passado para a função computar como um `` dict`` e o dict não incluirá o valor atual, ou qualquer outro campo computacional.
- `` authorize`` pode ser usado para requerer controle de acesso no campo correspondente, somente para campos "upload". Será discutido mais detalhadamente no contexto de Autenticação e Autorização.
- `` autodelete`` determina se o arquivo enviado por upload deve ser deletado quando o registro referenciando o arquivo for deletado. Apenas para campos "upload". No entanto, os registros excluídos pelo próprio banco de dados devido a uma operação CASCADE não acionarão o autodelete do web2py. O grupo do Web2py Google tem discussões sobre soluções alternativas.
- `` represent`` pode ser None ou pode apontar para uma função que recebe um valor de campo e retorna uma representação alternativa para o valor do campo.
-------
Exemplos:
``
db.mytable.name.represent = lambda name, row: name.capitalize()
db.mytable.other_id.represent = lambda id, row: row.myfield
db.mytable.some_uploadfield.represent = lambda value, row: A('get it', _href=URL('download', args=value))
``:code
-------


[[field_types]]
#### Tipos de campo
``field types``:inxx

----------
**field type** | **default field validators**
``string`` | ``IS_LENGTH(length)`` default length is 512
``text`` | ``IS_LENGTH(65536)``
``blob`` | ``None``
``boolean`` | ``None``
``integer`` | ``IS_INT_IN_RANGE(-1e100, 1e100)``
``double`` | ``IS_FLOAT_IN_RANGE(-1e100, 1e100)``
``decimal(n,m)`` | ``IS_DECIMAL_IN_RANGE(-1e100, 1e100)``
``date`` | ``IS_DATE()``
``time`` | ``IS_TIME()``
``datetime`` | ``IS_DATETIME()``
``password`` | ``None``
``upload`` | ``None``
``reference <table>``  | ``IS_IN_DB(db, table.field, format)``
``list:string`` | ``None``
``list:integer`` | ``None``
``list:reference <table>`` | ``IS_IN_DB(db, table.field, format, multiple=True)``
``json`` | ``IS_JSON()``
``bigint`` | ``None``
``big-id`` | ``None``
``big-reference`` | ``None``
---------
Decimal requer e retorna valores como objetos `` Decimal``, conforme definido no módulo `` decimal`` do Python. O SQLite não manipula o tipo `` decimal``, então internamente o tratamos como `` double``. O (n, m) é o número de dígitos no total e o número de dígitos após o ponto decimal, respectivamente.

O `` big-id`` e `` big-reference`` são suportados apenas por alguns dos mecanismos de banco de dados e são experimentais. Eles não são normalmente usados ​​como tipos de campos, a menos que para tabelas legadas, no entanto, o construtor DAL tem um argumento `` bigint_id`` que quando configurado para `` True`` torna os campos `` id`` e `` reference`` `` big-id`` e `` big-reference`` respectivamente.

Os campos `` list: <type> `` são especiais porque são projetados para aproveitar certos recursos de desnormalização no NoSQL (no caso do NoSQL do Google App Engine, os tipos de campo `` ListProperty`` e `` StringListProperty`` ) e back-port-los todos os outros bancos de dados relacionais suportados. Em bancos de dados relacionais, as listas são armazenadas como um campo `` text``. Os itens são separados por um `` | `` e cada `` | `` no item string é escapado como `` || ``. Eles são discutidos em sua própria seção.

O tipo de campo `` json`` é praticamente explicativo. Pode armazenar qualquer objeto serializável json. Ele foi projetado para funcionar especificamente para o MongoDB e backported para outros adaptadores de banco de dados para portabilidade.

``blob``:inxx
 Campos ``blob`` também são especiais. Por padrão, os dados binários são codificados em base64 antes de serem armazenados no campo real do banco de dados e são decodificados quando extraídos. Isso tem o efeito negativo de usar 33% mais espaço de armazenamento do que o necessário em campos de blob, mas tem a vantagem de tornar a comunicação independente de convenções de escape específicas de backend.

#### Campo de tempo de execução e modificação de tabela

A maioria dos atributos de campos e tabelas pode ser modificada depois de serem definidos:

``
db.define_table('person',
                Field('name', default=''),
                format='%(name)s')

db.person._format = '%(name)s/%(id)s'
db.person.name.default = 'anonymous'
``
(observe que os atributos das tabelas geralmente são prefixados por um sublinhado para evitar conflitos com possíveis nomes de campos).

Você pode listar as tabelas que foram definidas para uma determinada conexão de banco de dados:

``tables``:inxx
``
>>> print db.tables
['person']
``:code

Você também pode listar os campos que foram definidos para uma determinada tabela:

``fields``:inxx
``
>>> print db.person.fields
['id', 'name']
``:code


Você pode consultar o tipo de uma tabela:

``Table``:inxx
``
>>> print type(db.person)
<class 'pydal.objects.Table'>
``:code
e você pode acessar uma tabela da conexão DAL usando:
``
>>> print type(db['person'])
<class 'pydal.objects.Table'>
``:code

Da mesma forma, você pode acessar campos de seu nome de várias maneiras equivalentes:
``
>>> print type(db.person.name)
<class 'pydal.objects.Field'>
>>> print type(db.person['name'])
<class 'pydal.objects.Field'>
>>> print type(db['person']['name'])
<class 'pydal.objects.Field'>
``:code

Dado um campo, você pode acessar os atributos definidos em sua definição:
``
>>> print db.person.name.type
string
>>> print db.person.name.unique
False
>>> print db.person.name.notnull
False
>>> print db.person.name.length
32
``:code

incluindo sua tabela pai, tablename e conexão pai:
``
>>> db.person.name._table == db.person
True
>>> db.person.name._tablename == 'person'
True
>>> db.person.name._db == db
True
``:code

Um campo também tem métodos. Alguns deles são usados para criar consultas e as veremos mais tarde.
Um método especial do objeto de campo é `` validate`` e chama os validadores para o campo.

``
print(db.person.name.validate('John'))
``
que retorna uma tupla `` (value, error) ``. `` error`` é `` None`` se a entrada passar na validação.



[[table_migrations]]
### Migrações

``migrations``:inxx

``define_table`` verifica se a tabela correspondente existe ou não. Caso contrário, gera o SQL para criá-lo e executa o SQL. Se a tabela existe, mas difere da que está sendo definida, ela gera o SQL para alterar a tabela e executá-la. Se um campo tiver alterado o tipo mas não o nome, ele tentará converter os dados (se você não quiser isso, precisará redefinir a tabela duas vezes, pela primeira vez, deixando o web2py soltar o campo removendo-o e a segunda vez adicionando o campo recém-definido para que o web2py possa criá-lo.). Se a tabela existir e corresponder à definição atual, ela será deixada sozinha. Em todos os casos, ele criará o objeto `` db.person`` que representa a tabela.

Nós nos referimos a esse comportamento como uma "migração". O web2py registra todas as migrações e tentativas de migração no arquivo "databases / sql.log".

O primeiro argumento de `` define_table`` é sempre o nome da tabela. Os outros argumentos sem nome são os campos (campo). A função também recebe um argumento de palavra-chave opcional chamado "migrate":
``
>>> db.define_table('person', Field('name'), migrate='person.table')
``:code
O valor da migração é o nome do arquivo (na pasta "databases" do aplicativo), em que o web2py armazena informações de migração interna para essa tabela.
Esses arquivos são muito importantes e nunca devem ser removidos enquanto as tabelas correspondentes existirem. Nos casos em que uma tabela foi descartada e o arquivo correspondente ainda existe, ela pode ser removida manualmente. Por padrão, a migração é definida como True. Isso faz com que o web2py gere o nome do arquivo a partir de um hash da string de conexão. Se migrate estiver configurado como False, a migração não será executada e web2py assumirá que a tabela existe no armazenamento de dados e conterá (pelo menos) os campos listados em `` define_table``.
A melhor prática é fornecer um nome explícito para a tabela de migração.

Pode não haver duas tabelas no mesmo aplicativo com o mesmo nome de arquivo de migração.

A classe DAL também recebe um argumento "migrate", que determina o valor padrão da migração para chamadas para `` define_table``. Por exemplo,
``
>>> db = DAL('sqlite://storage.sqlite', migrate=False)
``:code

irá definir o valor padrão de migração para False sempre que `` db.define_table`` for chamada sem um argumento de migração.

------
Observe que o web2py migra apenas novas colunas, colunas removidas e alterações no tipo de coluna (exceto no sqlite). O web2py não migra mudanças em atributos como mudanças nos valores de `` default``, `` unique``, `` notnull`` e `` ondelete``.
------

As migrações podem ser desativadas para todas as tabelas de uma só vez:

``
db = DAL(..., migrate_enabled=False)
``

Esse é o comportamento recomendado quando dois aplicativos compartilham o mesmo banco de dados. Apenas um dos dois aplicativos deve realizar migrações, o outro deve desativá-los.

### Corrigindo migrações quebradas
``fake_migrate``:inxx
Existem dois problemas comuns com as migrações e há maneiras de recuperá-los.

Um problema é específico com o SQLite. O SQLite não impõe tipos de colunas e não pode descartar colunas. Isso significa que, se você tiver uma coluna do tipo string e removê-la, ela não será realmente removida. Se você adicionar a coluna novamente com um tipo diferente (por exemplo, datetime), você acabará com uma coluna datetime que contenha strings (lixo para propósitos práticos). web2py não se queixa disso porque não sabe o que está no banco de dados, até tentar recuperar registros e falhar.

Se web2py retornar um erro na função gluon.sql.parse ao selecionar registros, esse é o problema: dados corrompidos em uma coluna devido ao problema acima.

A solução consiste em atualizar todos os registros da tabela e atualizar os valores na coluna em questão com Nenhum.

O outro problema é mais genérico, mas típico do MySQL. O MySQL não permite mais de um ALTER TABLE em uma transação. Isso significa que o web2py deve quebrar transações complexas em transações menores (uma ALTER TABLE no momento) e confirmar uma parte no momento. Portanto, é possível que parte de uma transação complexa seja confirmada e uma parte falhe, deixando o web2py em um estado corrompido. Por que parte de uma transação falharia? Como, por exemplo, envolve a alteração de uma tabela e a conversão de uma coluna de string em uma coluna datetime, o web2py tenta converter os dados, mas os dados não podem ser convertidos. O que acontece com o web2py? Ele fica confuso sobre o que exatamente é a estrutura da tabela realmente armazenada no banco de dados.

A solução consiste em permitir migrações falsas:
``
db.define_table(...., migrate=True, fake_migrate=True)
``:code

Isso reconstruirá os metadados web2py sobre a tabela de acordo com a definição da tabela. Tente várias definições de tabela para ver qual delas funciona (aquela antes da migração com falha e a que ocorreu após a migração com falha). Uma vez bem sucedido, remova o parâmetro `` fake_migrate = True``.

Antes de tentar consertar os problemas de migração, é prudente fazer uma cópia dos arquivos "applications / yourapp / databases / *. Table".

Os problemas de migração também podem ser corrigidos para todas as tabelas de uma só vez:

``
db = DAL(..., fake_migrate_all=True)
``:code

Isso também falhará se o modelo descrever tabelas que não existem no banco de dados,
mas pode ajudar a reduzir o problema.

### Resumo do controle de migração

A lógica dos vários argumentos de migração está resumida neste pseudo-código:
``
if DAL.migrate_enabled and table.migrate:
   if DAL.fake_migrate_all or table.fake_migrate:
       perform fake migration
   else:
       perform migration
``:code

### ``insert``

Dada uma tabela, você pode inserir registros

``insert``:inxx
``
>>> db.person.insert(name="Alex")
1
>>> db.person.insert(name="Bob")
2
``:code

Insert retorna o valor "id" exclusivo de cada registro inserido.

Você pode truncar a tabela, ou seja, excluir todos os registros e redefinir o contador do ID.

``truncate``:inxx
``
>>> db.person.truncate()
``:code

Agora, se você inserir um registro novamente, o contador começa novamente em 1 (isso é específico do back-end e não se aplica ao Google NoSQL):
``
>>> db.person.insert(name="Alex")
1
``:code

Note que você pode passar parâmetros para `` truncate``, por exemplo, você pode dizer ao SQLITE para reiniciar o contador de IDs.

``
db.person.truncate('RESTART IDENTITY CASCADE')
``:code

O argumento está no SQL bruto e, portanto, específico do mecanismo.

``bulk_insert``:inxx
web2py also provides a bulk_insert method
``
>>> db.person.bulk_insert([{'name': 'Alex'}, {'name': 'John'}, {'name': 'Tim'}])
[3, 4, 5]
``:code

Ele pega uma lista de dicionários de campos a serem inseridos e executa várias inserções de uma só vez. Retorna os IDs dos registros inseridos. Nos bancos de dados relacionais suportados, não há vantagem em usar essa função, em vez de fazer o loop e executar inserções individuais, mas no NoSQL do Google App Engine, há uma grande vantagem de velocidade.

### ``commit`` e ``rollback``

As operações de inserção, truncamento, exclusão e atualização não são realmente consolidadas até que o web2py emita o comando commit. As operações de criação e eliminação podem ser executadas imediatamente, dependendo do mecanismo do banco de dados. Chamadas para ações da web2py são automaticamente agrupadas em transações. Se você executou comandos através do shell, você é obrigado a confirmar manualmente:

``commit``:inxx
``
>>> db.commit()
``:code

Para verificar isso, vamos inserir um novo registro:
``
>>> db.person.insert(name="Bob")
2
``:code

e reverter, ou seja, ignorar todas as operações desde o último commit:

``rollback``:inxx
``
>>> db.rollback()
``:code
Se você agora inserir novamente, o contador será novamente definido como 2, pois a inserção anterior foi revertida.
``
>>> db.person.insert(name="Bob")
2
``:code

O código em modelos, visualizações e controladores é incluído no código web2py que se parece com isso (pseudo código):
``
try:
    execute models, controller function and view
except:
    rollback all connections
    log the traceback
    send a ticket to the visitor
else:
    commit all connections
    save cookies, sessions and return the page
``:code

Portanto, em modelos, visualizações e controladores, não é necessário chamar `` commit`` ou `` rollback`` explicitamente em web2py, a menos que você precise de um controle mais granular.
Entretanto, nos módulos você precisará usar `` commit () ``.
### SQL bruto

#### Tempo das consultas

Todas as consultas são cronometradas automaticamente pelo web2py. A variável `` db._timings`` é uma lista de tuplas. Cada tupla contém a consulta SQL bruta conforme passada para o driver do banco de dados e o tempo que levou para executar em segundos. Essa variável pode ser exibida em visualizações usando a barra de ferramentas:
``
{{=response.toolbar()}}
``

#### ``executesql``

O DAL permite que você emita explicitamente instruções SQL.

``executesql``:inxx
``
>>> print db.executesql('SELECT * FROM person;')
[(1, u'Massimo'), (2, u'Massimo')]
``:code

Nesse caso, os valores de retorno não são analisados ou transformados pelo DAL e o formato depende do driver de banco de dados específico. Esse uso com selects normalmente não é necessário, mas é mais comum com índices.
`` executesql`` aceita quatro argumentos opcionais: `` placeholders``, `` as_dict``, `` fields`` e `` colnames``.
`` placeholders`` é um opcional
seqüência de valores a serem substituídos
ou, se suportado pelo driver DB, um dicionário com chaves
correspondência de espaços reservados nomeados no seu SQL.

Se `` as_dict`` estiver definido como True, o cursor de resultados retornado pelo driver de banco de dados será convertido em uma sequência de dicionários digitados com os nomes de campo de banco de dados. Resultados retornados com `` as_dict = True`` são os mesmos que foram retornados ao aplicar **. As_list () ** a uma seleção normal.
``
[{field1: value1, field2: value2}, {field1: value1b, field2: value2b}]
``:code
O argumento `` fields`` é uma lista de objetos DAL Field que correspondem ao
campos retornados do banco de dados. Os objetos de campo devem fazer parte de um ou
mais objetos de tabela definidos no objeto DAL. A lista `` fields`` pode
incluir um ou mais objetos da Tabela DAL além ou em vez de
incluindo objetos Field, ou pode ser apenas uma única tabela (não em um
Lista). Nesse caso, os objetos Field serão extraídos do
tabela (s).

Em vez de especificar o argumento `` fields``, o argumento `` colnames``
pode ser especificado como uma lista de nomes de campos no formato tablename.fieldname.
Novamente, estes devem representar tabelas e campos definidos no DAL
objeto.

Também é possível especificar ambos os `` fields`` e os associados
`` colnames``. Nesse caso, `` fields`` também pode incluir a expressão DAL
objetos além dos objetos Field. Para objetos de campo em "campos",
os `` colnames`` associados ainda devem estar no formato tablename.fieldname.
Para objetos Expression em `` fields``, o `` colnames`` associado pode
quaisquer rótulos arbitrários.

Observe que os objetos da Tabela DAL referenciados por `` fields`` ou `` colnames`` podem
ser tabelas fictícias e não tem que representar quaisquer tabelas reais no
base de dados. Além disso, observe que os `` fields`` e `` colnames`` devem estar no
mesma ordem que os campos no cursor de resultados retornados do banco de dados.

#### ``_lastsql``

Se o SQL foi executado manualmente usando o executesql ou o SQL gerado pelo DAL, você sempre pode encontrar o código SQL em `` db._lastsql``. Isso é útil para fins de depuração:

``_lastdb``:inxx
``
>>> rows = db().select(db.person.ALL)
>>> print db._lastsql
SELECT person.id, person.name FROM person;
``:code

-------
O web2py nunca gera consultas usando o operador "*". O web2py é sempre explícito ao selecionar campos.
-------

### ``drop``

Finalmente, você pode excluir tabelas e todos os dados serão perdidos:

``drop``:inxx
``
>>> db.person.drop()
``:code

Nota para o sqlite: o web2py não recriará a tabela removida até que você navegue no sistema de arquivos para o diretório de bancos de dados do seu aplicativo e exclua o arquivo associado à tabela eliminada.


### Índices

Atualmente, a API DAL não fornece um comando para criar índices em tabelas, mas isso pode ser feito usando o comando `` executesql``. Isso ocorre porque a existência de índices pode tornar as migrações complexas e é melhor lidar com elas explicitamente. Os índices podem ser necessários para os campos usados em consultas recorrentes.

Aqui está um exemplo de como [[criar um índice usando SQL no SQLite http://www.sqlite.org/lang_createindex.html]]:
``
>>> db = DAL('sqlite://storage.sqlite')
>>> db.define_table('person', Field('name'))
>>> db.executesql('CREATE INDEX IF NOT EXISTS myidx ON person (name);')
``:code

Outros dialetos de banco de dados têm sintaxes muito semelhantes, mas podem não suportar a diretiva "IF NOT EXISTS" opcional.

[[LegacyDatabases]]
### Bancos de dados legados e tabelas com chaves

O web2py pode se conectar a bancos de dados legados sob algumas condições.

A maneira mais fácil é quando essas condições são atendidas:
- Cada tabela deve ter um campo inteiro de incremento automático exclusivo chamado "id"
- Os registros devem ser referenciados exclusivamente usando o campo "id".

Ao acessar uma tabela existente, ou seja, uma tabela não criada pelo web2py no aplicativo atual, sempre defina `` migrate = False``.

Se a tabela legada tem um campo inteiro de auto-incremento mas não é chamado de "id", web2py ainda pode acessá-la, mas a definição da tabela deve conter explicitamente como `` Field ('...', 'id') `` onde ... é o nome do campo inteiro de incremento automático.

``keyed table``:inxx

Finalmente, se a tabela legada usar uma chave primária que não seja um campo de ID de incremento automático, é possível usar uma "tabela com chave", por exemplo:

``
db.define_table('account',
                Field('accnum', 'integer'),
                Field('acctype'),
                Field('accdesc'),
                primarykey=['accnum', 'acctype'],
                migrate=False)
``:code
- `` primarykey`` é uma lista dos nomes de campos que compõem a chave primária.
- Todos os campos primários têm um conjunto `` NOT NULL`` mesmo se não forem especificados.
- Tabelas com chave só podem referenciar outras tabelas com chave.
- Os campos de referência devem usar o formato `` reference tablename.fieldname``.
- A função `` update_record`` não está disponível para Rows de tabelas com esse tipo de primary key.

-------
Atualmente, tabelas com chave são suportadas apenas para DB2, MS-SQL, Ingres e Informix, mas outros mecanismos serão incluídos.
-------

No momento da escrita, não podemos garantir que o atributo `` primarykey`` funcione com todas as tabelas legadas existentes e com todos os back-ends de banco de dados suportados.
Para simplificar, recomendamos, se possível, criar uma exibição de banco de dados que tenha um campo de ID de incremento automático.

### Transação Distribuída
``distributed transactions``:inxx
------
No momento de escrever este recurso só é suportado
pelo PostgreSQL, MySQL e Firebird, já que expõem API para commits de duas fases.
------

Supondo que você tenha duas (ou mais) conexões com bancos de dados PostgreSQL distintos, por exemplo:
``
db_a = DAL('postgres://...')
db_b = DAL('postgres://...')
``:code

Em seus modelos ou controladores, você pode fazer um commit simultâneo com:
``
DAL.distributed_transaction_commit(db_a, db_b)
``:code

Em caso de falha, esta função reverte e gera um `` Exception``.

Nos controladores, quando uma ação é retornada, se você tiver duas conexões distintas e não chamar a função acima, o web2py as submete separadamente. Isso significa que existe a possibilidade de um dos commits ser bem-sucedido e um deles falhar. A transação distribuída impede que isso aconteça.

### Mais sobre uploads

Considere o seguinte modelo:
``
>>> db.define_table('myfile',
                    Field('image', 'upload', default='path/'))
``:code

No caso de um campo 'upload', o valor padrão pode opcionalmente ser definido para um caminho (um caminho absoluto ou um caminho relativo à pasta do aplicativo atual) e a imagem padrão será definida para uma cópia do arquivo no caminho . Uma nova cópia é feita para cada novo registro que não especifica uma imagem.

Normalmente, uma inserção é manipulada automaticamente por meio de um formulário SQLFORM ou crud (que é um SQLFORM), mas ocasionalmente você já tem o arquivo no sistema de arquivos e deseja carregá-lo programaticamente. Isso pode ser feito desta maneira:

``
>>> stream = open(filename, 'rb')
>>> db.myfile.insert(image=db.myfile.image.store(stream, filename))
``:code
Também é possível inserir um arquivo de maneira mais simples e ter o armazenamento de chamadas do método insert automaticamente:

``
>>> stream = open(filename, 'rb')
>>> db.myfile.insert(image=stream)
``:code

Nesse caso, o nome do arquivo é obtido do objeto de fluxo, se disponível.

O método `` store`` do objeto de campo de upload usa um fluxo de arquivos e um nome de arquivo. Ele usa o nome do arquivo para determinar a extensão (tipo) do arquivo, cria um novo nome temporário para o arquivo (de acordo com o mecanismo de upload web2py) e carrega o conteúdo do arquivo nesse novo arquivo temporário (na pasta uploads, a menos que especificado de outra forma). Ele retorna o novo nome temp, que é então armazenado no campo `` image`` da tabela `` db.myfile``.

Note, se o arquivo deve ser armazenado em um campo blob associado ao invés do sistema de arquivos, o método `` store () `` não irá inserir o arquivo no campo blob (porque `` store () `` é chamado antes a inserção), então o arquivo deve ser explicitamente inserido no campo blob:
``
>>> db.define_table('myfile',
                    Field('image', 'upload', uploadfield='image_file'),
                    Field('image_file', 'blob'))
>>> stream = open(filename, 'rb')
>>> db.myfile.insert(image=db.myfile.image.store(stream, filename),
                     image_file=stream.read())
``:code


O oposto de `` .store`` é `` .retrieve``:

``
>>> row = db(db.myfile).select().first()
>>> (filename, stream) = db.myfile.image.retrieve(row.image)
>>> import shutil
>>> shutil.copyfileobj(stream, open(filename, 'wb'))
``

### ``Query``, ``Set``, ``Rows``

Vamos considerar novamente a tabela definida (e descartada) anteriormente e inserir três registros:

``
>>> db.define_table('person', Field('name'))
>>> db.person.insert(name="Alex")
1
>>> db.person.insert(name="Bob")
2
>>> db.person.insert(name="Carl")
3
``:code

Você pode armazenar a tabela em uma variável. Por exemplo, com a variável `` person``, você poderia fazer:

``Table``:inxx
``
>>> person = db.person
``:code

Você também pode armazenar um campo em uma variável como `` name``. Por exemplo, você também pode fazer:

``Field``:inxx
``
>>> name = person.name
``:code

Você pode até construir uma consulta (usando operadores como ==,! =, <,>, <=,> =, Como, pertence) e armazenar a consulta em uma variável `` q`` como em:

``Query``:inxx
``
>>> q = name == 'Alex'
``:code

Quando você chama `` db`` com uma consulta, você define um conjunto de registros. Você pode armazená-lo em uma variável `` s`` e escrever:

``Set``:inxx
``
>>> s = db(q)
``:code

Observe que nenhuma consulta de banco de dados foi realizada até o momento. DAL + Query simplesmente define um conjunto de registros neste banco de dados que corresponde à consulta.
O web2py determina a partir da consulta quais tabelas (ou tabelas) estão envolvidas e, de fato, não há necessidade de especificar isso.

### ``select``

Dado um Set, `` s``, você pode buscar os registros com o comando `` select``:

``Rows``:inxx ``select``:inxx
``
>>> rows = s.select()
``:code

``Row``:inxx
Ele retorna um objeto iterável da classe `` pydal.objects.Rows`` cujos elementos são objetos Row. Objetos `` pydal.objects.Row`` agem como dicionários, mas seus elementos também podem ser acessados como atributos, como `` gluon.storage.Storage``. Os primeiros diferem dos últimos porque seus valores são somente leitura.

O objeto Rows permite efetuar o loop sobre o resultado da seleção e imprimir os valores de campo selecionados para cada linha:
``
>>> for row in rows:
        print row.id, row.name
1 Alex
``:code

Você pode fazer todos os passos em uma declaração:
``
>>> for row in db(db.person.name == 'Alex').select():
        print row.name
Alex
``:code

``ALL``:inxx

O comando select pode receber argumentos. Todos os argumentos sem nome são interpretados como os nomes dos campos que você deseja buscar. Por exemplo, você pode ser explícito ao buscar o campo "id" e o campo "name":
``
>>> for row in db().select(db.person.id, db.person.name):
        print row.name
Alex
Bob
Carl
``:code

O atributo table ALL permite especificar todos os campos:
``
>>> for row in db().select(db.person.ALL):
        print row.name
Alex
Bob
Carl
``:code

Observe que não há nenhuma string de consulta passada para o db. web2py entende que se você quiser todos os campos da pessoa da tabela sem informações adicionais, então você quer todos os registros da tabela 'person'.

Uma sintaxe alternativa equivalente é a seguinte:
``
>>> for row in db(db.person.id > 0).select():
        print row.name
Alex
Bob
Carl
``:code

e web2py entende que se você pedir todos os registros da pessoa da tabela (id> 0) sem informações adicionais, então você quer todos os campos da pessoa da tabela.

Dada uma linha

``
row = rows[0]
``

você pode extrair seus valores usando várias expressões equivalentes:

``
>>> row.name
Alex
>>> row['name']
Alex
>>> row('person.name')
Alex
``

A última sintaxe é particularmente útil ao selecionar a expressão en em vez de uma coluna. Nós vamos mostrar isso mais tarde.

Você também pode fazer
``
rows.compact = False
``
para desativar a notação
``
row[i].name
``
e habilitar, em vez disso, a notação menos compacta:
``
row[i].person.name
``
Sim, isso é incomum e raramente é necessário.

Objetos de linha também possuem dois métodos importantes:

``
row.delete_record()
``
e
``
row.update_record(name="new value")
``

#### Using an iterator-based select for lower memory use

Python "iterators" are a type of "lazy-evaluation". They 'feed' data one step at time; traditional python loops create the entrire set of data in memory before looping. 

The traditional use of select is:
``
for row in db(db.table.id > 0).select():
    rtn = row
``:code

but for large numbers of rows, using an iterator-based alternative has dramatically lower memory use:
``
for row in db(db.table.id > 0).iterselect():
    rtn = row
``:code
Testing shows this is around 10% faster as well, even on machines with large RAM.


#### Rendering rows using represent
You may wish to rewrite rows returned by select to take advantage of formatting information contained in the represents setting of the fields. 

``
rows = db(query).select()  
repr_row = rows.render(0)
``:code

If you don't specify an index, you get a generator to iterate over all the rows:

``
for row in rows.render():
    print row.myfield
``:code

Can also be applied to slices:

``
for row in rows[0:10].render():
    print row.myfield
``:code

If you only want to transform selected fields via their "represent" attribute, you can list them in the "fields" argument:

``
repr_row = row.render(0, fields=[db.mytable.myfield])
``:code

Note, it returns a transformed copy of the original Row, so there's no update_record (which you wouldn't want anyway) or delete_record.


#### Shortcuts
``DAL shortcuts``:inxx

The DAL supports various code-simplifying shortcuts.
In particular:
``
myrecord = db.mytable[id]
``:code

returns the record with the given ``id`` if it exists. If the ``id`` does not exist, it returns ``None``. The above statement is equivalent to

``
myrecord = db(db.mytable.id == id).select().first()
``:code


You can delete records by id:

``
del db.mytable[id]
``:code

and this is equivalent to

``
db(db.mytable.id == id).delete()
``:code

and deletes the record with the given ``id``, if it exists.

Note: This delete shortcut syntax does not currently work if [[versioning #versioning]] is activated

You can insert records:

``
db.mytable[0] = dict(myfield='somevalue')
``:code

It is equivalent to

``
db.mytable.insert(myfield='somevalue')
``:code

and it creates a new record with field values specified by the dictionary on the right hand side.

You can update records:

``
db.mytable[id] = dict(myfield='somevalue')
``:code

which is equivalent to

``
db(db.mytable.id == id).update(myfield='somevalue')
``:code

and it updates an existing record with field values specified by the dictionary on the right hand side.

#### Fetching a ``Row``

Yet another convenient syntax is the following:

``
record = db.mytable(id)
record = db.mytable(db.mytable.id == id)
record = db.mytable(id, myfield='somevalue')
``:code

Apparently similar to ``db.mytable[id]`` the above syntax is more flexible and safer. First of all it checks whether ``id`` is an int (or ``str(id)`` is an int) and returns ``None`` if not (it never raises an exception). It also allows to specify multiple conditions that the record must meet. If they are not met, it also returns ``None``.

#### Recursive ``select``s
``recursive selects``:inxx

Consider the previous table person and a new table "thing" referencing a "person":
``
>>> db.define_table('thing',
                    Field('name'),
                    Field('owner_id', 'reference person'))
``:code

and a simple select from this table:
``
>>> things = db(db.thing).select()
``:code

which is equivalent to

``
>>> things = db(db.thing._id > 0).select()
``:code

where ``._id`` is a reference to the primary key of the table. Normally ``db.thing._id`` is the same as ``db.thing.id`` and we will assume that in most of this book. ``_id``:inxx


For each Row of things it is possible to fetch not just fields from the selected table (thing) but also from linked tables (recursively):
``
>>> for thing in things: print thing.name, thing.owner_id.name
``:code

Here ``thing.owner_id.name`` requires one database select for each thing in things and it is therefore inefficient. We suggest using joins whenever possible instead of recursive selects, nevertheless this is convenient and practical when accessing individual records.

You can also do it backwards, by selecting the things referenced by a person:

``
person =  db.person(id)
for thing in person.thing.select(orderby=db.thing.name):
    print person.name, 'owns', thing.name
``:code

In this last expression ``person.thing`` is a shortcut for

``
db(db.thing.owner_id == person.id)
``:code

i.e. the Set of ``thing``s referenced by the current ``person``. This syntax breaks down if the referencing table has multiple references to the referenced table. In this case one needs to be more explicit and use a full Query.

[[sqltable]]
#### Serializing ``Rows`` in views

Given the following action containing a query
``SQLTABLE``:inxx

``
def index():
    return dict(rows = db(query).select())
``:code

The result of a select can be displayed in a view with the following syntax:
``
{{extend 'layout.html'}}
<h1>Records</h1>
{{=rows}}
``:code

Which is equivalent to:
``
{{extend 'layout.html'}}
<h1>Records</h1>
{{=SQLTABLE(rows)}}
``:code

``SQLTABLE`` converts the rows into an HTML table with a header containing the column names and one row per record. The rows are marked as alternating class "even" and class "odd". Under the hood, Rows is first converted into a SQLTABLE object (not to be confused with Table) and then serialized. The values extracted from the database are also formatted by the validators associated to the field and then escaped.

Yet it is possible and sometimes convenient to call SQLTABLE explicitly.

The SQLTABLE constructor takes the following optional arguments:

- ``linkto`` lambda function or an action to be used to link reference fields (default to None).
If you assign it a string with the name of an action, it will generate a link to that function passing it, as args, the name of the table and the id of each record (in this order). Example:
``
linkto = 'pointed_function' # generates something like <a href="pointed_function/table_name/id_value">
``:code
If you want a different link to be generated, you can specify a lambda, wich will receive as parameters, the value of the id, the type of the object (e.g. table), and the name of the object. For example, if you want to receive the args in reverse order:
``
linkto = lambda id, type, name: URL(f='pointed_function', args=[id, name])
``:code
- ``upload`` the URL or the download action to allow downloading of uploaded files (default to None)
- ``headers`` a dictionary mapping field names to their labels to be used as headers (default to ``{}``). It can also be an instruction. Currently we support ``headers='fieldname:capitalize'``.
- ``truncate`` the number of characters for truncating long values in the table (default is 16)
- ``columns`` the list of fieldnames to be shown as columns (in tablename.fieldname format).
   Those not listed are not displayed (defaults to all).
- ``**attributes`` generic helper attributes to be passed to the most external TABLE object.

Here is an example:
``
{{extend 'layout.html'}}
<h1>Records</h1>
{{=SQLTABLE(rows,
            headers='fieldname:capitalize',
            truncate=100,
            upload=URL('download'))
}}
``:code

``SQLFORM.grid``:inxx ``SQLFORM.smartgrid``:inxx

------
``SQLTABLE`` is useful but there are times when one needs more. ``SQLFORM.grid`` is an extension of SQLTABLE that creates a table with search features and pagination, as well as ability to open detailed records, create, edit and delete records. ``SQLFORM.smartgrid`` is a further generalization that allows all of the above but also creates buttons to access referencing records.
------

Here is an example of usage of ``SQLFORM.grid``:

``
def index():
    return dict(grid=SQLFORM.grid(query))
``:code

and the corresponding view:

``
{{extend 'layout.html'}}
{{=grid}}
``

For working with multiple rows, ``SQLFORM.grid`` and ``SQLFORM.smartgrid`` are preferred to ``SQLTABLE`` because they are more powerful. Please see chapter 7.

[[orderby]] [[limitby]] [[distinct]]
#### ``orderby``, ``groupby``, ``limitby``, ``distinct``, ``having``,``orderby_on_limitby``,``left``,``cache``

The ``select`` command takes a number of optional arguments.

##### orderby
You can fetch the records sorted by name:

``orderby``:inxx ``groupby``:inxx ``having``:inxx
``
>>> for row in db().select(
        db.person.ALL, orderby=db.person.name):
        print row.name
Alex
Bob
Carl
``:code

You can fetch the records sorted by name in reverse order (notice the tilde):
``
>>> for row in db().select(db.person.ALL, orderby=~db.person.name):
        print row.name
Carl
Bob
Alex
``:code

You can have the fetched records appear in random order:
``
>>> for row in db().select(db.person.ALL, orderby='<random>'):
        print row.name
Carl
Alex
Bob
``:code

-----
The use of ``orderby='<random>'`` is not supported on Google NoSQL.  However, in this situation and likewise in many others where built-ins are insufficient, imports can be used:
``
import random
rows=db(...).select().sort(lambda row: random.random())
``:code
-----

You can sort the records according to multiple fields by concatenating them with a "|":
``
>>> for row in db().select(db.person.ALL, orderby=db.person.name|db.person.id):
        print row.name
Carl
Bob
Alex
``:code

##### groupby, having
Using ``groupby`` together with ``orderby``, you can group records with the same value for the specified field (this is back-end specific, and is not on the Google NoSQL):
``
>>> for row in db().select(db.person.ALL,
                           orderby=db.person.name,
                           groupby=db.person.name):
        print row.name
Alex
Bob
Carl
``:code

You can use ``having`` in conjunction with ``groupby`` to group conditionally (only those ``having`` the condition are grouped.

``
>>> print db(query1).select(db.person.ALL, groupby=db.person.name, having=query2)
``

Notice that query1 filters records to be displayed, query2 filters records to be grouped.

##### distinct
``distinct``:inxx

With the argument ``distinct=True``, you can specify that you only want to select distinct records. This has the same effect as grouping using all specified fields except that it does not require sorting. When using distinct it is important not to select ALL fields, and in particular not to select the "id" field, else all records will always be distinct.

Here is an example:
``
>>> for row in db().select(db.person.name, distinct=True):
        print row.name
Alex
Bob
Carl
``:code

Notice that ``distinct`` can also be an expression for example:
``
>>> for row in db().select(db.person.name, distinct=db.person.name):
        print row.name
Alex
Bob
Carl
``:code
##### limitby
With limitby=(min, max), you can select a subset of the records from offset=min to but not including offset=max (in this case, the first two starting at zero):

``limitby``:inxx
``
>>> for row in db().select(db.person.ALL, limitby=(0, 2)):
        print row.name
Alex
Bob
``:code

##### orderby_on_limitby
``orderby_on_limitby``:inxx
Note that the DAL defaults to implicitly adding an orderby when using a limitby.
This ensures the same query returns the same results each time, important for pagination.
But it can cause performance problems. 
use ``orderby_on_limitby = False`` to change this (this defaults to True). 

##### left
Discussed below in the section on joins

##### cache, cacheable
An example use which gives much faster selects is:
``rows = db(query).select(cache=(cache.ram, 3600), cacheable=True)``:code
See discussion on 'caching selects', below, to understand what the trade-offs are.


#### Logical operators

Queries can be combined using the binary AND operator "``&``":

``and``:inxx ``or``:inxx ``not``:inxx
``
>>> rows = db((db.person.name=='Alex') & (db.person.id>3)).select()
>>> for row in rows: print row.id, row.name
4 Alex
``:code

and the binary OR operator "``|``":
``
>>> rows = db((db.person.name == 'Alex') | (db.person.id > 3)).select()
>>> for row in rows:
        print row.id, row.name
1 Alex
``:code

You can negate a query (or sub-query) with the "``!=``" binary operator:
``
>>> rows = db((db.person.name != 'Alex') | (db.person.id > 3)).select()
>>> for row in rows:
        print row.id, row.name
2 Bob
3 Carl
``:code

or by explicit negation with the "``~``" unary operator:
``
>>> rows = db(~(db.person.name == 'Alex') | (db.person.id > 3)).select()
>>> for row in rows:
        print row.id, row.name
2 Bob
3 Carl
``:code

------
Due to Python restrictions in overloading "``and``" and "``or``" operators, these cannot be used in forming queries.  The binary operators "``&``" and "``|``" must be used instead. Note that these operators (unlike "``and``" and "``or``") have higher precedence than comparison operators, so the "extra" parentheses in the above examples are mandatory. Similarly, the unary operator "``~``" has higher precedence than comparison operators, so ``~``-negated comparisons must also be parenthesized.
------

It is also possible to build queries using in-place logical operators:

``
>>> query = db.person.name != 'Alex'
>>> query &= db.person.id > 3
>>> query |= db.person.name == 'John'
``

#### ``count``, ``isempty``, ``delete``, ``update``

You can count records in a set:

``count``:inxx ``isempty``:inxx

``
>>> print db(db.person.id > 0).count()
3
``:code

Notice that ``count`` takes an optional ``distinct`` argument which defaults to False, and it works very much like the same argument for ``select``. ``count`` has also a ``cache`` argument that works very much like the equivalent argument of the ``select`` method.

Sometimes you may need to check if a table is empty. A more efficient way than counting is using the ``isempty`` method:

``
>>> print db(db.person.id > 0).isempty()
False
``:code

or equivalently:

``
>>> print db(db.person).isempty()
False
``:code

You can delete records in a set:

``delete``:inxx
``
>>> db(db.person.id > 3).delete()
``:code

And you can update all records in a set by passing named arguments corresponding to the fields that need to be updated:

``update``:inxx
``
>>> db(db.person.id > 3).update(name='Ken')
``:code

The ``update`` method returns the number of records that were updated (0 if no records were updated).

#### Expressions

The value assigned an update statement can be an expression. For example consider this model
``
>>> db.define_table('person',
                    Field('name'),
                    Field('visits', 'integer', default=0))

>>> db(db.person.name == 'Massimo').update(visits = db.person.visits + 1)
``:code

The values used in queries can also be expressions
``
>>> db.define_table('person',
                    Field('name'),
                    Field('visits', 'integer', default=0),
                    Field('clicks', 'integer', default=0))

>>> db(db.person.visits == db.person.clicks + 1).delete()
``:code

#### ``case`` ``case``:inxx

An expression can contain a case clause for example:

``
>>> db.define_table('person', Field('name'))
>>> condition = db.person.name.startswith('M')
>>> yes_or_no = condition.case('Yes', 'No')
>>> for row in db().select(db.person.name, yes_or_no):
...     print row.person.name,  row(yes_or_no)
Max Yes
John No
``:code

#### ``update_record``

``update_record``:inxx
web2py also allows updating a single record that is already in memory using ``update_record``

``
>>> row = db(db.person.id == 2).select().first()
>>> row.update_record(name='Curt')
``:code

``update_record`` should not be confused with

``
>>> row.update(name='Curt')
``:code

because for a single row, the method ``update`` updates the row object but not the database record, as in the case of ``update_record``.

It is also possible to change the attributes of a row (one at a time) and then call ``update_record()`` without arguments to save the changes:

``
>>> row = db(db.person.id > 2).select().first()
>>> row.name = 'Curt'
>>> row.update_record() # saves above change
``:code

-------
Note, you should avoid using ``row.update_record()`` with no arguments when the ``row`` object contains fields that have an ``update`` attribute (e.g., ``Field('modified_on', update=request.now)``). Calling ``row.update_record()`` will retain ''all'' of the existing values in the ``row`` object, so any fields with ``update`` attributes will have no effect in this case. Be particularly mindful of this with tables that include ``auth.signature``.
-------

The ``update_record`` method is available only if the table's ``id`` field is included in the select, and ``cacheable`` is not set to ``True``.

#### Inserting and updating from a dictionary

A common issue consists of needing to insert or update records in a table where the name of the table, the field to be updated, and the value for the field are all stored in variables. For example: ``tablename``, ``fieldname``, and ``value``.

The insert can be done using the following syntax:

``
db[tablename].insert(**{fieldname:value})
``:code

The update of record with given id can be done with: ``_id``:inxx

``
db(db[tablename]._id == id).update(**{fieldname:value})
``:code

Notice we used ``table._id`` instead of ``table.id``. In this way the query works even for tables with a field of type "id" which has a name other than "id".


#### ``first`` and ``last``
``first``:inxx ``last``:inxx

Given a Rows object containing records:

``
>>> rows = db(query).select()
>>> first_row = rows.first()
>>> last_row = rows.last()
``:code

are equivalent to
``
>>> first_row = rows[0] if len(rows)>0 else None
>>> last_row = rows[-1] if len(rows)>0 else None
``:code

Notice, ``first()`` and ``last()`` allow you to obtain obviously the first and last record present in your query, but this won't mean that these records are going to be the first or last inserted records. In case you want the first or last record inputted in a given table don't forget to use ``orderby=db.table_name.id``. If you forget you will only get the first and last record returned by your query which are often in a random order determined by the backend query optimiser.

#### ``as_dict`` and ``as_list``
``as_list``:inxx ``as_dict``:inxx

A Row object can be serialized into a regular dictionary using the ``as_dict()`` method and a Rows object can be serialized into a list of dictionaries using the ``as_list()`` method. Here are some examples:
``
>>> rows = db(query).select()
>>> rows_list = rows.as_list()
>>> first_row_dict = rows.first().as_dict()
``:code

These methods are convenient for passing Rows to generic views and or to store Rows in sessions (since Rows objects themselves cannot be serialized since contain a reference to an open DB connection):
``
>>> rows = db(query).select()
>>> session.rows = rows  # not allowed!
>>> session.rows = rows.as_list()  # allowed!
``:code

#### Combining rows

Row objects can be combined at the Python level. Here we assume:

``
>>> print rows1
person.name
Max
Tim
>>> print rows2
person.name
John
Tim
``


You can do union of the records in two sets of rows:

``
>>> rows3 = rows1 & rows2
>>> print rows3
name
Max
Tim
John
Tim
``:code

You can do a union of the records removing duplicates:

``
>>> rows3 = rows1 | rows2
>>> print rows3
name
Max
Tim
John
``:code

#### ``find``, ``exclude``, ``sort``
``find``:inxx ``exclude``:inxx ``sort``:inxx

Some times you need to perform two selects and one contains a subset of a previous select. In this case it is pointless to access the database again. The ``find``, ``exclude`` and ``sort`` objects allow you to manipulate a Rows object and generate another one without accessing the database. More specifically:
- ``find`` returns a new set of Rows filtered by a condition and leaves the original unchanged.
- ``exclude`` returns a new set of Rows filtered by a condition and removes them from the original Rows.
- ``sort`` returns a new set of Rows sorted by a condition and leaves the original unchanged.

All these methods take a single argument, a function that acts on each individual row.

Here is an example of usage:
``
>>> db.define_table('person', Field('name'))
>>> db.person.insert(name='John')
>>> db.person.insert(name='Max')
>>> db.person.insert(name='Alex')
>>> rows = db(db.person).select()
>>> for row in rows.find(lambda row: row.name[0]=='M'):
        print row.name
Max
>>> print len(rows)
3
>>> for row in rows.exclude(lambda row: row.name[0]=='M'):
        print row.name
Max
>>> print len(rows)
2
>>> for row in rows.sort(lambda row: row.name):
        print row.name
Alex
John
``:code

They can be combined:
``
>>> rows = db(db.person).select()
>>> rows = rows.find(lambda row: 'x' in row.name
                     ).sort(lambda row: row.name)
>>> for row in rows:
        print row.name
Alex
Max
``:code

Sort takes an optional argument ``reverse=True`` with the obvious meaning.

The ``find`` method has an optional limitby argument with the same syntax and functionality as the Set select ``method``.



### Other methods

#### ``update_or_insert``
``update_or_insert``:inxx

Some times you need to perform an insert only if there is no record with the same values as those being inserted.
This can be done with

``
db.define_table('person',
                Field('name'),
                Field('birthplace'))

db.person.update_or_insert(name='John', birthplace='Chicago')
``:code

The record will be inserted only if there is no other user called John born in Chicago.

You can specify which values to use as a key to determine if the record exists. For example:
``
db.person.update_or_insert(db.person.name == 'John',
                           name='John',
                           birthplace='Chicago')
``:code

and if there is John his birthplace will be updated else a new record will be created.

The selection criteria in the example above is a single field. 
It can also be a query, such as 
``
db.person.update_or_insert((db.person.name == 'John') & (db.person.birthplace == 'Chicago'),
                           name='John',
                           birthplace='Chicago',
                           pet='Rover')
``:code

#### ``validate_and_insert``, ``validate_and_update``

``validate_and_insert``:inxx ``validate_and_update``:inxx

The function

``
ret = db.mytable.validate_and_insert(field='value')
``:code

works very much like

``
id = db.mytable.insert(field='value')
``:code

except that it calls the validators for the fields before performing the insert and bails out if the validation does not pass. If validation does not pass the errors can be found in ``ret.errors``. ``ret.errors`` holds a key-value mapping where each key is the field name whose validation failed, and the value of the key is the result from the validation error (much like ``form.errors``). If it passes, the id of the new record is in ``ret.id``. Mind that normally validation is done by the form processing logic so this function is rarely needed.

Similarly

``
ret = db(query).validate_and_update(field='value')
``:code

works very much the same as

``
num = db(query).update(field='value')
``:code

except that it calls the validators for the fields before performing the update. Notice that it only works if query involves a single table. The number of updated records can be found in ``ret.updated`` and errors will be ``ret.errors``.

#### ``smart_query`` (experimental)

There are times when you need to parse a query using natural language such as

``
name contain m and age greater than 18
``

The DAL provides a method to parse this type of queries:

``
search = 'name contain m and age greater than 18'
rows = db.smart_query([db.person], search).select()
``

The first argument must be a list of tables or fields that should be allowed in the search. It raises a ``RuntimeError`` if the search string is invalid. This functionality can be used to build RESTful interfaces (see chapter 10) and it is used internally by the ``SQLFORM.grid`` and ``SQLFORM.smartgrid``.

In the smartquery search string, a field can be identified by fieldname only and or by tablename.fieldname. Strings may be delimited by double quotes if they contain spaces.

### Computed fields
``compute``:inxx

DAL fields may have a ``compute`` attribute. This must be a function (or lambda) that takes a Row object and returns a value for the field. When a new record is modified, including both insertions and updates, if a value for the field is not provided, web2py tries to compute from the other field values using the ``compute`` function. Here is an example:
``
>>> db.define_table('item',
                    Field('unit_price', 'double'),
                    Field('quantity', 'integer'),
                    Field('total_price',
                          compute=lambda r: r['unit_price'] * r['quantity']))

>>> r = db.item.insert(unit_price=1.99, quantity=5)
>>> print r.total_price
9.95
``:code

Notice that the computed value is stored in the db and it is not computed on retrieval, as in the case of virtual fields, described later. Two typical applications of computed fields are:
- in wiki applications, to store the processed input wiki text as HTML, to avoid re-processing on every request
- for searching, to compute normalized values for a field, to be used for searching.

Computed fields are evaluated in the order in which they are defined in the table definition. A computed field can refer to previously defined computed fields (new after v 2.5.1)

### Virtual fields

``virtual fields``:inxx

Virtual fields are also computed fields (as in the previous subsection) but they differ from those because they are ''virtual'' in the sense that they are not stored in the db and they are computed each time records are extracted from the database. They can be used to simplify the user's code without using additional storage but they cannot be used for searching.

#### New style virtual fields

web2py provides a new and easier way to define virtual fields and lazy virtual fields. This section is marked experimental because they APIs may still change a little from what is described here.

Here we will consider the same example as in the previous subsection. In particular we consider the following model:

``
>>> db.define_table('item',
                    Field('unit_price', 'double'),
                    Field('quantity', 'integer'))
``:code

One can define a ``total_price`` virtual field as

``
>>> db.item.total_price = \
        Field.Virtual('total_price',
                      lambda row: row.item.unit_price * row.item.quantity)
``:code

i.e. by simply defining a new field ``total_price`` to be a ``Field.Virtual``. The only argument of the constructor is a function that takes a row and returns the computed values.

A virtual field defined as the one above is automatically computed for all records when the records are selected:

``
>>> for row in db(db.item).select():
        print row.total_price
``

It is also possible to define method fields which are calculated on-demand, when called.
For example:

``
>>> db.item.discounted_total = \
        Field.Method(lambda row, discount=0.0:
                     row.item.unit_price * row.item.quantity * (1.0 - discount / 100))
``:code

In this case ``row.discounted_total`` is not a value but a function. The function takes the same arguments as the function passed to the ``Method`` constructor except for ``row`` which is implicit (think of it as ``self`` for rows objects).

The lazy field in the example above allows one to compute the total price for each ``item``:

``
>>> for row in db(db.item).select(): print row.discounted_total()
``

And it also allows to pass an optional ``discount`` percentage (15%):

``
>>> for row in db(db.item).select():
        print row.discounted_total(15)
``

Virtual and Method fields can also be defined in place when a table is defined:

``
>>> db.define_table('item',
                    Field('unit_price', 'double'),
                    Field('quantity', 'integer'),
                    Field.Virtual('total_price',
                                  lambda row: ...),
                    Field.Method('discounted_total',
                                 lambda row, discount=0.0: ...))
``:code


------
Mind that virtual fields do not have the same attributes as the other fields (default, readable, requires, etc).  In older versions of web2py they do not appear in the list of ``db.table.fields`` and they require a special approach to display in SQLFORM.grid and SQLFORM.smartgrid. See the discussion on grids and virtual fields in the Forms chapter.
------

#### Old style virtual fields

In order to define one or more virtual fields, you can also define a container class, instantiate it and link it to a table or to a select. For example, consider the following table:

``
>>> db.define_table('item',
                    Field('unit_price', 'double'),
                    Field('quantity', 'integer'))
``:code

One can define a ``total_price`` virtual field as
``
>>> class MyVirtualFields(object):
        def total_price(self):
            return self.item.unit_price*self.item.quantity
>>> db.item.virtualfields.append(MyVirtualFields())
``:code

Notice that each method of the class that takes a single argument (self) is a new virtual field. ``self`` refers to each one row of the select. Field values are referred by full path as in ``self.item.unit_price``. The table is linked to the virtual fields by appending an instance of the class to the table's ``virtualfields`` attribute.

Virtual fields can also access recursive fields as in
``
>>> db.define_table('item',
                    Field('unit_price', 'double'))

>>> db.define_table('order_item',
                    Field('item', 'reference item'),
                    Field('quantity', 'integer'))
>>> class MyVirtualFields(object):
        def total_price(self):
            return self.order_item.item.unit_price * self.order_item.quantity

>>> db.order_item.virtualfields.append(MyVirtualFields())
``:code

Notice the recursive field access ``self.order_item.item.unit_price`` where ``self`` is the looping record.

They can also act on the result of a JOIN
``
>>> db.define_table('item',
                    Field('unit_price', 'double'))

>>> db.define_table('order_item',
                    Field('item', 'reference item'),
                    Field('quantity', 'integer'))

>>> rows = db(db.order_item.item == db.item.id).select()
>>> class MyVirtualFields(object):
        def total_price(self):
            return self.item.unit_price * self.order_item.quantity

>>> rows.setvirtualfields(order_item=MyVirtualFields())

>>> for row in rows:
        print row.order_item.total_price
``:code

Notice how in this case the syntax is different. The virtual field accesses both ``self.item.unit_price`` and ``self.order_item.quantity`` which belong to the join select. The virtual field is attached to the rows of the table using the ``setvirtualfields`` method of the rows object. This method takes an arbitrary number of named arguments and can be used to set multiple virtual fields, defined in multiple classes, and attach them to multiple tables:
``
>>> class MyVirtualFields1(object):
        def discounted_unit_price(self):
            return self.item.unit_price * 0.90
>>> class MyVirtualFields2(object):
        def total_price(self):
            return self.item.unit_price * self.order_item.quantity

        def discounted_total_price(self):
            return self.item.discounted_unit_price * self.order_item.quantity

>>> rows.setvirtualfields(item=MyVirtualFields1(),
                          order_item=MyVirtualFields2())

>>> for row in rows:
        print row.order_item.discounted_total_price
``:code

Virtual fields can be ''lazy''; all they need to do is return a function and access it by calling the function:
``
>>> db.define_table('item',
                    Field('unit_price', 'double'),
                    Field('quantity', 'integer'))

>>> class MyVirtualFields(object):
        def lazy_total_price(self):
            def lazy(self=self):
                return self.item.unit_price * self.item.quantity
            return lazy

>>> db.item.virtualfields.append(MyVirtualFields())

>>> for item in db(db.item).select():
        print item.lazy_total_price()
``:code

or shorter using a lambda function:
``
>>> class MyVirtualFields(object):
        def lazy_total_price(self):
            return lambda self=self: self.item.unit_price * self.item.quantity
``:code

### One to many relation
``one to many``:inxx

To illustrate how to implement one to many relations with the web2py DAL, define another table "thing" that refers to the table "person" which we redefine here:
``
>>> db.define_table('person',
                    Field('name'),
                    format='%(name)s')

>>> db.define_table('thing',
                    Field('name'),
                    Field('owner_id', 'reference person'),
                    format='%(name)s')
``:code

Table "thing" has two fields, the name of the thing and the owner of the thing. The "owner_id" field id a reference field. A reference type can be specified in two equivalent ways:

``
Field('owner_id', 'reference person')
Field('owner_id', db.person)
``:code

The latter is always converted to the former. They are equivalent except in the case of lazy tables, self references or other types of cyclic references where the former notation is the only allowed notation.

When a field type is another table, it is intended that the field reference the other table by its id. In fact, you can print the actual type value and get:
``
>>> print db.thing.owner_id.type
reference person
``:code

Now, insert three things, two owned by Alex and one by Bob:
``
>>> db.thing.insert(name='Boat', owner_id=1)
1
>>> db.thing.insert(name='Chair', owner_id=1)
2
>>> db.thing.insert(name='Shoes', owner_id=2)
3
``:code

You can select as you did for any other table:
``
>>> for row in db(db.thing.owner_id == 1).select():
        print row.name
Boat
Chair
``:code

Because a thing has a reference to a person, a person can have many things, so a record of table person now acquires a new attribute thing, which is a Set, that defines the things of that person. This allows looping over all persons and fetching their things easily:

``referencing``:inxx
``
>>> for person in db().select(db.person.ALL):
        print person.name
        for thing in person.thing.select():
            print '    ', thing.name
Alex
     Boat
     Chair
Bob
     Shoes
Carl
``:code

#### Inner joins

Another way to achieve a similar result is by using a join, specifically an INNER JOIN. web2py performs joins automatically and transparently when the query links two or more tables as in the following example:

``Rows``:inxx ``inner join``:inxx ``join``:inxx
``
>>> rows = db(db.person.id == db.thing.owner_id).select()

>>> for row in rows:
        print row.person.name, 'has', row.thing.name

Alex has Boat
Alex has Chair
Bob has Shoes
``:code

Observe that web2py did a join, so the rows now contain two records, one from each table, linked together. Because the two records may have fields with conflicting names, you need to specify the table when extracting a field value from a row. This means that while before you could do:
``
row.name
``:code

and it was obvious whether this was the name of a person or a thing, in the result of a join you have to be more explicit and say:
``
row.person.name
``:code

or:
``
row.thing.name
``:code

There is an alternative syntax for INNER JOINS:
``
>>> rows = db(db.person).select(join=db.thing.on(db.person.id == db.thing.owner_id))

>>> for row in rows:
    print row.person.name, 'has', row.thing.name

Alex has Boat
Alex has Chair
Bob has Shoes
``:code

While the output is the same, the generated SQL in the two cases can be different. The latter syntax removes possible ambiguities when the same table is joined twice and aliased:

``
>>> db.define_table('thing',
                    Field('name'),
                    Field('owner_id1', 'reference person'),
                    Field('owner_id2', 'reference person'))

>>> rows = \
        db(db.person).select(join=[db.person.with_alias('owner_id1').on(db.person.id == db.thing.owner_id1),
                                   db.person.with_alias('owner_id2').on(db.person.id == db.thing.owner_id2)])
``

The value of ``join`` can be list of ``db.table.on(...)`` to join.

#### Left outer join

Notice that Carl did not appear in the list above because he has no things. If you intend to select on persons (whether they have things or not) and their things (if they have any), then you need to perform a LEFT OUTER JOIN. This is done using the argument "left" of the select command. Here is an example:

``Rows``:inxx ``left outer join``:inxx ``outer join``:inxx
``
>>> rows = db().select(db.person.ALL, db.thing.ALL,
                       left=db.thing.on(db.person.id == db.thing.owner_id))

>>> for row in rows:
        print row.person.name, 'has', row.thing.name

Alex has Boat
Alex has Chair
Bob has Shoes
Carl has None
``:code

where:
``
left = db.thing.on(...)
``:code

does the left join query. Here the argument of ``db.thing.on`` is the condition required for the join (the same used above for the inner join). In the case of a left join, it is necessary to be explicit about which fields to select.

Multiple left joins can be combined by passing a list or tuple of ``db.mytable.on(...)`` to the  ``left`` attribute.

#### Grouping and counting

When doing joins, sometimes you want to group rows according to certain criteria and count them. For example, count the number of things owned by every person. web2py allows this as well. First, you need a count operator. Second, you want to join the person table with the thing table by owner. Third, you want to select all rows (person + thing), group them by person, and count them while grouping:

``grouping``:inxx
``
>>> count = db.person.id.count()
>>> for row in db(db.person.id == db.thing.owner_id
                  ).select(db.person.name, count, groupby=db.person.name):
        print row.person.name, row[count]

Alex 2
Bob 1
``:code

Notice the ``count`` operator (which is built-in) is used as a field. The only issue here is in how to retrieve the information. Each row clearly contains a person and the count, but the count is not a field of a person nor is it a table. So where does it go? It goes into the storage object representing the record with a key equal to the query expression itself. The count method of the Field object has an optional ``distinct`` argument. When set to ``True`` it specifies that only distinct values of the field in question are to be counted.

### Many to many
``many-to-many``:inxx
In the previous examples, we allowed a thing to have one owner but one person could have many things. What if Boat was owned by Alex and Curt? This requires a many-to-many relation, and it is realized via an intermediate table that links a person to a thing via an ownership relation.

Here is how to do it:
``
>>> db.define_table('person',
                    Field('name'))

>>> db.define_table('thing',
                    Field('name'))

>>> db.define_table('ownership',
                    Field('person', 'reference person'),
                    Field('thing', 'reference thing'))
``:code

the existing ownership relationship can now be rewritten as:
``
>>> db.ownership.insert(person=1, thing=1)  # Alex owns Boat
>>> db.ownership.insert(person=1, thing=2)  # Alex owns Chair
>>> db.ownership.insert(person=2, thing=3)  # Bob owns Shoes

``:code

Now you can add the new relation that Curt co-owns Boat:
``
>>> db.ownership.insert(person=3, thing=1)  # Curt owns Boat too

``:code

Because you now have a three-way relation between tables, it may be convenient to define a new set on which to perform operations:
``
>>> persons_and_things = db((db.person.id == db.ownership.person) &
                            (db.thing.id == db.ownership.thing))
``:code

Now it is easy to select all persons and their things from the new Set:
``
>>> for row in persons_and_things.select():
        print row.person.name, row.thing.name

Alex Boat
Alex Chair
Bob Shoes
Curt Boat
``:code

Similarly, you can search for all things owned by Alex:
``
>>> for row in persons_and_things(db.person.name == 'Alex').select():
        print row.thing.name

Boat
Chair
``:code

and all owners of Boat:
``
>>> for row in persons_and_things(db.thing.name == 'Boat').select():
        print row.person.name

Alex
Curt
``:code

A lighter alternative to Many 2 Many relations is tagging. Tagging is discussed in the context of the ``IS_IN_DB`` validator. Tagging works even on database backends that do not support JOINs like the Google App Engine NoSQL.

### ``list:<type>`` and ``contains``
``list:string``:inxx
``list:integer``:inxx
``list:reference``:inxx
``contains``:inxx
``multiple``:inxx
``tags``:inxx

web2py provides the following special field types:

``
list:string
list:integer
list:reference <table>
``:code

They can contain lists of strings, of integers and of references respectively.

On Google App Engine NoSQL ``list:string`` is mapped into ``StringListProperty``, the other two are mapped into ``ListProperty(int)``. On relational databases they are mapped into text fields which contain the list of items separated by ``|``. For example ``[1, 2, 3]`` is mapped into ``|1|2|3|``.

For lists of string the items are escaped so that any ``|`` in the item is replaced by a ``||``. Anyway this is an internal representation and it is transparent to the user.

You can use ``list:string``, for example, in the following way:

``
>>> db.define_table('product',
                    Field('name'),
                    Field('colors', 'list:string'))

>>> db.product.colors.requires=IS_IN_SET(('red', 'blue', 'green'))

>>> db.product.insert(name='Toy Car', colors=['red', 'green'])

>>> products = db(db.product.colors.contains('red')).select()

>>> for item in products:
        print item.name, item.colors

Toy Car ['red', 'green']
``:code

``list:integer`` works in the same way but the items must be integers.

As usual the requirements are enforced at the level of forms, not at the level of ``insert``.

------
For ``list:<type>`` fields the ``contains(value)`` operator maps into a non trivial query that checks for lists containing the ``value``.  The ``contains`` operator also works for regular ``string`` and ``text`` fields and it maps into a ``LIKE '%value%'``.
------

The ``list:reference`` and the ``contains(value)`` operator are particularly useful to de-normalize many-to-many relations. Here is an example:

``
>>> db.define_table('tag',
                    Field('name'),
                    format='%(name)s')

>>> db.define_table('product',
                    Field('name'),
                    Field('tags', 'list:reference tag'))

>>> a = db.tag.insert(name='red')

>>> b = db.tag.insert(name='green')

>>> c = db.tag.insert(name='blue')

>>> db.product.insert(name='Toy Car', tags=[a, b, c])

>>> products = db(db.product.tags.contains(b)).select()

>>> for item in products:
        print item.name, item.tags

Toy Car [1, 2, 3]

>>> for item in products:
        print item.name, db.product.tags.represent(item.tags)

Toy Car red, green, blue
``:code

Notice that a ``list:reference tag`` field get a default constraint

``
requires = IS_IN_DB(db, 'tag.id', db.tag._format, multiple=True)
``:code

that produces a ``SELECT/OPTION`` multiple drop-box in forms.

Also notice that this field gets a default ``represent`` attribute which represents the list of references as a comma-separated list of formatted references. This is used in read forms and ``SQLTABLE``s.

-----
While ``list:reference`` has a default validator and a default representation, ``list:integer`` and ``list:string`` do not. So these two need an ``IS_IN_SET`` or an ``IS_IN_DB`` validator if you want to use them in forms.
-----


### Other operators

web2py has other operators that provide an API to access equivalent SQL operators.
Let's define another table "log" to store security events, their event_time and severity, where the severity is an integer number.

``date``:inxx ``datetime``:inxx ``time``:inxx
``
>>> db.define_table('log', Field('event'),
                           Field('event_time', 'datetime'),
                           Field('severity', 'integer'))
``:code

As before, insert a few events, a "port scan", an "xss injection" and an "unauthorized login".
For the sake of the example, you can log events with the same event_time but with different severities (1, 2, and 3 respectively).
``
>>> import datetime
>>> now = datetime.datetime.now()
>>> print db.log.insert(
        event='port scan', event_time=now, severity=1)
1
>>> print db.log.insert(
        event='xss injection', event_time=now, severity=2)
2
>>> print db.log.insert(
        event='unauthorized login', event_time=now, severity=3)
3
``:code

#### ``like``, ``ilike``, ``regexp``, ``startswith``, ``endswith``, ``contains``, ``upper``, ``lower``

``like``:inxx ``ilike``:inxx ``startswith``:inxx ``endswith``:inxx ``regexp``:inxx
``contains``:inxx ``upper``:inxx ``lower``:inxx

Fields have a like operator that you can use to match strings:

``
>>> for row in db(db.log.event.like('port%')).select():
        print row.event
port scan
``:code

Here "port%" indicates a string starting with "port". The percent sign character, "%", is a wild-card character that means "any sequence of characters".

The like operator maps to the LIKE word in ANSI-SQL. LIKE is case-sensitive in most databases, and depends on the collation of the database itself. The ``like`` method is hence case-sensitive but it can be made case-insensitive with

``
db.mytable.myfield.like('value', case_sensitive=False)
``:code


web2py also provides some shortcuts:

``
db.mytable.myfield.startswith('value')
db.mytable.myfield.endswith('value')
db.mytable.myfield.contains('value')
``:code

which are roughly equivalent respectively to

``
db.mytable.myfield.like('value%')
db.mytable.myfield.like('%value')
db.mytable.myfield.like('%value%')
``:code

Notice that ``contains`` has a special meaning for ``list:<type>`` fields and it was discussed in a previous section.

The ``contains`` method can also be passed a list of values and an optional boolean argument ``all`` to search for records that contain all values:

``
db.mytable.myfield.contains(['value1', 'value2'], all=True)
``
or any value from the list
``
db.mytable.myfield.contains(['value1', 'value2'], all=False)
``

There is a also a ``regexp`` method that works like the ``like`` method but allows regular expression syntax for the look-up expression. It is only supported by PostgreSQL, MySQL, Oracle and SQLite (with different degree of support).

The ``upper`` and ``lower`` methods allow you to convert the value of the field to upper or lower case, and you can also combine them with the like operator:

``upper``:inxx ``lower``:inxx
``
>>> for row in db(db.log.event.upper().like('PORT%')).select():
        print row.event

port scan
``:code

#### ``year``, ``month``, ``day``, ``hour``, ``minutes``, ``seconds``
``hour``:inxx ``minutes``:inxx ``seconds``:inxx ``day``:inxx ``month``:inxx ``year``:inxx

The date and datetime fields have day, month and year methods. The datetime and time fields have hour, minutes and seconds methods. Here is an example:

``
>>> for row in db(db.log.event_time.year() == 2013).select():
        print row.event

port scan
xss injection
unauthorized login
``:code

#### ``belongs``

The SQL IN operator is realized via the belongs method which returns true when the field value belongs to the specified set (list or tuples):

``belongs``:inxx
``
>>> for row in db(db.log.severity.belongs((1, 2))).select():
        print row.event

port scan
xss injection
``:code

The DAL also allows a nested select as the argument of the belongs operator. The only caveat is that the nested select has to be a ``_select``, not a ``select``, and only one field has to be selected explicitly, the one that defines the set.

``nested select``:inxx
``
>>> bad_days = db(db.log.severity == 3)._select(db.log.event_time)

>>> for row in db(db.log.event_time.belongs(bad_days)).select():
        print row.event

port scan
xss injection
unauthorized login
``:code

In those cases where a nested select is required and the look-up field is a reference we can also use a query as argument. For example:

``
db.define_table('person', Field('name'))
db.define_table('thing',
                Field('name'),
                Field('owner_id', 'reference thing'))

db(db.thing.owner_id.belongs(db.person.name == 'Jonathan')).select()
``:code

In this case it is obvious that the next select only needs the field referenced by the ``db.thing.owner_id`` field so we do not need the more verbose ``_select`` notation.

``nested_select``:inxx

A nested select can also be used as insert/update value but in this case the syntax is different:

``
lazy = db(db.person.name == 'Jonathan').nested_select(db.person.id)

db(db.thing.id == 1).update(owner_id = lazy)
``:code

In this case ``lazy`` is a nested expression that computes the ``id`` of person "Jonathan". The two lines result in one single SQL query.

#### ``sum``, ``avg``, ``min``, ``max`` and ``len``

``sum``:inxx ``avg``:inxx ``min``:inxx ``max``:inxx
Previously, you have used the count operator to count records. Similarly, you can use the sum operator to add (sum) the values of a specific field from a group of records. As in the case of count, the result of a sum is retrieved via the store object:
``
>>> sum = db.log.severity.sum()
>>> print db().select(sum).first()[sum]
6
``:code

You can also use ``avg``, ``min``, and ``max`` to the average, minimum, and maximum value respectively for the selected records. For example:

``
>>> max = db.log.severity.max()
>>> print db().select(max).first()[max]
3
``:code

``.len()`` computes the length of a string, text or boolean fields.

Expressions can be combined to form more complex expressions. For example here we are computing the sum of the length of all the severity strings in the logs, increased of one:

``
>>> sum = (db.log.severity.len() + 1).sum()
>>> print db().select(sum).first()[sum]
``:code

#### Substrings

One can build an expression to refer to a substring. For example, we can group things whose name starts with the same three characters and select only one from each group:

``
db(db.thing).select(distinct = db.thing.name[:3])
``:code


#### Default values with ``coalesce`` and ``coalesce_zero``

There are times when you need to pull a value from database but also need a default values if the value for a record is set to NULL. In SQL there is a keyword, ``COALESCE``, for this. web2py has an equivalent ``coalesce`` method:

``
>>> db.define_table('sysuser', Field('username'), Field('fullname'))
>>> db.sysuser.insert(username='max', fullname='Max Power')
>>> db.sysuser.insert(username='tim', fullname=None)
print db(db.sysuser).select(db.sysuser.fullname.coalesce(db.sysuser.username))
"COALESCE(sysuser.fullname, sysuser.username)"
Max Power
tim
``

Other times you need to compute a mathematical expression but some fields have a value set to None while it should be zero.
``coalesce_zero`` comes to the rescue by defaulting None to zero in the query:

``
>>> db.define_table('sysuser', Field('username'), Field('points'))
>>> db.sysuser.insert(username='max', points=10)
>>> db.sysuser.insert(username='tim', points=None)
>>> print db(db.sysuser).select(db.sysuser.points.coalesce_zero().sum())
"SUM(COALESCE(sysuser.points,0))"
10
``

### Generating raw sql
``raw SQL``:inxx

Sometimes you need to generate the SQL but not execute it. This is easy to do with web2py since every command that performs database IO has an equivalent command that does not, and simply returns the SQL that would have been executed. These commands have the same names and syntax as the functional ones, but they start with an underscore:

Here is ``_insert`` ``_insert``:inxx
``
>>> print db.person._insert(name='Alex')
INSERT INTO person(name) VALUES ('Alex');
``:code

Here is ``_count`` ``_count``:inxx
``
>>> print db(db.person.name == 'Alex')._count()
SELECT count(*) FROM person WHERE person.name='Alex';
``:code

Here is ``_select`` ``_select``:inxx
``
>>> print db(db.person.name == 'Alex')._select()
SELECT person.id, person.name FROM person WHERE person.name='Alex';
``:code

Here is ``_delete`` ``_delete``:inxx
``
>>> print db(db.person.name == 'Alex')._delete()
DELETE FROM person WHERE person.name='Alex';
``:code

And finally, here is ``_update`` ``_update``:inxx
``
>>> print db(db.person.name == 'Alex')._update()
UPDATE person SET  WHERE person.name='Alex';
``:code

-----
Moreover you can always use ``db._lastsql`` to return the most recent
SQL code, whether it was executed manually using executesql or was SQL
generated by the DAL.
-----

### Exporting and importing data
``export``:inxx ``import``:inxx

#### CSV (one Table at a time)

When a Rows object is converted to a string it is automatically
serialized in CSV:

``csv``:inxx
``
>>> rows = db(db.person.id == db.thing.owner_id).select()
>>> print rows

person.id, person.name, thing.id, thing.name, thing.owner_id
1, Alex, 1, Boat, 1
1, Alex, 2, Chair, 1
2, Bob, 3, Shoes, 2
``:code

You can serialize a single table in CSV and store it in a file "test.csv":
``
>>> open('test.csv', 'wb').write(str(db(db.person.id).select()))
``:code

This is equivalent to

``
>>> rows = db(db.person.id).select()
>>> rows.export_to_csv_file(open('test.csv', 'wb'))
``:code

You can read the CSV file back with:
``
>>> db.person.import_from_csv_file(open('test.csv', 'r'))
``:code

When importing, web2py looks for the field names in the CSV header. In this example, it finds two columns: "person.id" and "person.name". It ignores the "person." prefix, and it ignores the "id" fields. Then all records are appended and assigned new ids. Both of these operations can be performed via the appadmin web interface.

#### CSV (all tables at once)

In web2py, you can backup/restore an entire database with two commands:

To export:
``
>>> db.export_to_csv_file(open('somefile.csv', 'wb'))
``:code

To import:
``
>>> db.import_from_csv_file(open('somefile.csv', 'rb'))
``:code

This mechanism can be used even if the importing database is of a different type than the exporting database. The data is stored in "somefile.csv" as a CSV file where each table starts with one line that indicates the tablename, and another line with the fieldnames:
``
TABLE tablename
field1, field2, field3, ...
``:code

Two tables are separated ``\r\n\r\n``. The file ends with the line
``
END
``:code

The file does not include uploaded files if these are not stored in the database. In any case it is easy enough to zip the "uploads" folder separately.

When importing, the new records will be appended to the database if it is not empty. In general the new imported records will not have the same record id as the original (saved) records but web2py will restore references so they are not broken, even if the id values may change.

If a table contains a field called
"uuid", this field will be used to identify duplicates.  Also, if an
imported record has the same "uuid" as an existing record, the
previous record will be updated.

#### CSV and remote database synchronization

Consider the following model:
``
db = DAL('sqlite:memory:')
db.define_table('person',
                Field('name'),
                format='%(name)s')
db.define_table('thing',
                Field('owner_id', 'reference person'),
                Field('name'),
                format='%(name)s')

if not db(db.person).count():
    id = db.person.insert(name="Massimo")
    db.thing.insert(owner_id=id, name="Chair")
``:code

Each record is identified by an ID and referenced by that ID. If you
have two copies of the database used by distinct web2py installations,
the ID is unique only within each database and not across the databases.
This is a problem when merging records from different databases.

In order to make a record uniquely identifiable across databases, they
must:
- have a unique id (UUID),
- have an event_time (to figure out which one is more recent if multiple copies),
- reference the UUID instead of the id.

This can be achieved without modifying web2py. Here is what to do:

Change the above model into:

``
db.define_table('person',
                Field('uuid', length=64, default=lambda:str(uuid.uuid4())),
                Field('modified_on', 'datetime', default=request.now),
                Field('name'),
                format='%(name)s')

db.define_table('thing',
                Field('uuid', length=64, default=lambda:str(uuid.uuid4())),
                Field('modified_on', 'datetime', default=request.now),
                Field('owner_id', length=64),
                Field('name'),
                format='%(name)s')

db.thing.owner_id.requires = IS_IN_DB(db, 'person.uuid', '%(name)s')

if not db(db.person.id).count():
    id = uuid.uuid4()
    db.person.insert(name="Massimo", uuid=id)
    db.thing.insert(owner_id=id, name="Chair")
``:code

-------
Notice that in the above table definitions, the default value for the two ``uuid`` fields is set to a lambda function, which returns a UUID (converted to a string). The lambda function is called once for each record inserted, ensuring that each record gets a unique UUID, even if multiple records are inserted in a single transaction.
-------

Create a controller action to export the database:

``
def export():
    s = StringIO.StringIO()
    db.export_to_csv_file(s)
    response.headers['Content-Type'] = 'text/csv'
    return s.getvalue()
``:code

Create a controller action to import a saved copy of the other database and sync records:

``
def import_and_sync():
    form = FORM(INPUT(_type='file', _name='data'), INPUT(_type='submit'))
    if form.process().accepted:
        db.import_from_csv_file(form.vars.data.file, unique=False)
        # for every table
        for table in db.tables:
            # for every uuid, delete all but the latest
            items = db(db[table]).select(db[table].id,
                                         db[table].uuid,
                                         orderby=db[table].modified_on,
                                         groupby=db[table].uuid)
            for item in items:
                db((db[table].uuid==item.uuid) &
                   (db[table].id!=item.id)).delete()
    return dict(form=form)
``:code

Optionally you should create an index manually to make the search by uuid faster.


``XML-RPC``:inxx
Alternatively, you can use XML-RPC to export/import the file.

If the records reference uploaded files, you also need to export/import the content of the uploads folder. Notice that files therein are already labeled by UUIDs so you do not need to worry about naming conflicts and references.

#### HTML and XML (one Table at a time)

``Rows objects``:inxx
Rows objects also have an ``xml`` method (like helpers) that serializes it to XML/HTML:

``HTML``:inxx

``
>>> rows = db(db.person.id > 0).select()
>>> print rows.xml()
<table>
  <thead>
    <tr>
      <th>person.id</th>
      <th>person.name</th>
      <th>thing.id</th>
      <th>thing.name</th>
      <th>thing.owner_id</th>
    </tr>
  </thead>
  <tbody>
    <tr class="even">
      <td>1</td>
      <td>Alex</td>
      <td>1</td>
      <td>Boat</td>
      <td>1</td>
    </tr>
    ...
  </tbody>
</table>
``:code

``Rows custom tags``:inxx
If you need to serialize the Rows in any other XML format with custom tags, you can easily do that using the universal TAG helper and the * notation:
``XML``:inxx

``
>>> rows = db(db.person.id > 0).select()
>>> print TAG.result(*[TAG.row(*[TAG.field(r[f], _name=f) for f in db.person.fields]) for r in rows])

<result>
  <row>
    <field name="id">1</field>
    <field name="name">Alex</field>
  </row>
  ...
</result>
``:code

#### Data representation

``export_to_csv_file``:inxx
The ``export_to_csv_file`` function accepts a keyword argument named ``represent``. When ``True`` it will use the columns ``represent`` function while exporting the data instead of the raw data.

``colnames``:inxx
The function also accepts a keyword argument named ``colnames`` that should contain a list of column names one wish to export. It defaults to all columns.

Both ``export_to_csv_file`` and ``import_from_csv_file`` accept keyword arguments that tell the csv parser the format to save/load the files:
- ``delimiter``: delimiter to separate values (default ',')
- ``quotechar``: character to use to quote string values (default to double quotes)
- ``quoting``: quote system (default ``csv.QUOTE_MINIMAL``)

Here is some example usage:
``
>>> import csv
>>> rows = db(query).select()
>>> rows.export_to_csv_file(open('/tmp/test.txt', 'w'),
                            delimiter='|',
                            quotechar='"',
                            quoting=csv.QUOTE_NONNUMERIC)
``:code

Which would render something similar to
``
"hello"|35|"this is the text description"|"2013-03-03"
``:code

For more information consult the official Python documentation ``quoteall``:cite

### Caching selects

The select method also takes a cache argument, which defaults to None. For caching purposes, it should be set to a tuple where the first element is the cache model (cache.ram, cache.disk, etc.), and the second element is the expiration time in seconds.

In the following example, you see a controller that caches a select on the previously defined db.log table. The actual select fetches data from the back-end database no more frequently than once every 60 seconds and stores the result in cache.ram. If the next call to this controller occurs in less than 60 seconds since the last database IO, it simply fetches the previous data from cache.ram.

``cache select``:inxx
``
def cache_db_select():
    logs = db().select(db.log.ALL, cache=(cache.ram, 60))
    return dict(logs=logs)
``:code

``cacheable``:inxx

The ``select`` method has an optional ``cacheable`` argument, normally set to ``False``. When ``cacheable=True`` the resulting ``Rows`` is serializable but The ``Row``s lack ``update_record`` and ``delete_record`` methods.

If you do not need these methods you can speed up selects a lot by setting the cacheable attribute:

``
rows = db(query).select(cacheable=True)
``:code

When the ``cache`` argument is set but ``cacheable=False`` (default) only the database results are cached, not the actual Rows object. When the ``cache`` argument is used in conjunction with ``cacheable=True`` the entire Rows object is cached and this results in much faster caching:

``
rows = db(query).select(cache=(cache.ram, 3600), cacheable=True)
``:code

### Self-Reference and aliases

``self reference``:inxx
``alias``:inxx
It is possible to define tables with fields that refer to themselves, here is an example:
``reference table``:inxx
``
db.define_table('person',
                Field('name'),
                Field('father_id', 'reference person'),
                Field('mother_id', 'reference person'))
``:code

Notice that the alternative notation of using a table object as field type will fail in this case, because it uses a variable ``db.person`` before it is defined:
``
db.define_table('person',
                Field('name'),
                Field('father_id', db.person),  # wrong!
                Field('mother_id', db.person))  # wrong!
``:code

In general ``db.tablename`` and ``"reference tablename"`` are equivalent field types, but the latter is the only one allowed for self.references.

``with_alias``:inxx
If the table refers to itself, then it is not possible to perform a JOIN to select a person and its parents without use of the SQL "AS" keyword. This is achieved in web2py using the ``with_alias``. Here is an example:
``
>>> Father = db.person.with_alias('father')
>>> Mother = db.person.with_alias('mother')
>>> db.person.insert(name='Massimo')
1
>>> db.person.insert(name='Claudia')
2
>>> db.person.insert(name='Marco', father_id=1, mother_id=2)
3
>>> rows = db().select(db.person.name, Father.name, Mother.name,
                       left=(Father.on(Father.id == db.person.father_id),
                             Mother.on(Mother.id == db.person.mother_id)))

>>> for row in rows:
        print row.person.name, row.father.name, row.mother.name

Massimo None None
Claudia None None
Marco Massimo Claudia
``:code

Notice that we have chosen to make a distinction between:
- "father_id": the field name used in the table "person";
- "father": the alias we want to use for the table referenced by the above field; this is communicated to the database;
- "Father": the variable used by web2py to refer to that alias.

The difference is subtle, and there is nothing wrong in using the same name for the three of them:
``
db.define_table('person',
                Field('name'),
                Field('father', 'reference person'),
                Field('mother', 'reference person'))

>>> father = db.person.with_alias('father')
>>> mother = db.person.with_alias('mother')
>>> db.person.insert(name='Massimo')
1
>>> db.person.insert(name='Claudia')
2
>>> db.person.insert(name='Marco', father=1, mother=2)
3
>>> rows = db().select(db.person.name, father.name, mother.name,
                       left=(father.on(father.id==db.person.father),
                             mother.on(mother.id==db.person.mother)))

>>> for row in rows:
        print row.person.name, row.father.name, row.mother.name

Massimo None None
Claudia None None
Marco Massimo Claudia
``:code

But it is important to have the distinction clear in order to build correct queries.

### Advanced features

#### Table inheritance
``inheritance``:inxx

It is possible to create a table that contains all the fields from another table. It is sufficient to pass the other table in place of a field to ``define_table``. For example
``
db.define_table('person', Field('name'))
db.define_table('doctor', db.person, Field('specialization'))
``:code

``dummy table``:inxx
It is also possible to define a dummy table that is not stored in a database in order to reuse it in multiple other places. For example:

``
signature = db.Table(db, 'signature',
                     Field('created_on', 'datetime', default=request.now),
                     Field('created_by', db.auth_user, default=auth.user_id),
                     Field('updated_on', 'datetime', update=request.now),
                     Field('updated_by', db.auth_user, update=auth.user_id))

db.define_table('payment', Field('amount', 'double'), signature)
``:code

This example assumes that standard web2py authentication is enabled.

Notice that if you use ``Auth`` web2py already creates one such table for you:

``
auth = Auth(db)
db.define_table('payment', Field('amount', 'double'), auth.signature)
``

When using table inheritance, if you want the inheriting table to inherit validators, be sure to define the validators of the parent table before defining the inheriting table.

#### ``filter_in`` and ``filter_out``
``filter_in``:inxx ``filter_out``:inxx

It is possible to define a filter for each field to be called before a value is inserted into the database for that field and after a value is retrieved from the database.

Imagine for example that you want to store a serializable Python data structure in a field in the json format. Here is how it could be accomplished:

``
>>> from simplejson import loads, dumps
>>> db.define_table('anyobj',
                    Field('name'),
                    Field('data', 'text'))

>>> db.anyobj.data.filter_in = lambda obj, dumps=dumps: dumps(obj)
>>> db.anyobj.data.filter_out = lambda txt, loads=loads: loads(txt)
>>> myobj = ['hello', 'world', 1, {2: 3}]
>>> id = db.anyobj.insert(name='myobjname', data=myobj)
>>> row = db.anyobj(id)
>>> row.data
['hello', 'world', 1, {2: 3}]
``:code

Another way to accomplish the same is by using a Field of type ``SQLCustomType``, as discussed later.

#### callbacks on record insert, delete and update

``_before_insert``:inxx
``_after_insert``:inxx
``_before_update``:inxx
``_after_update``:inxx
``_before_delete``:inxx
``_after_delete``:inxx

Web2py provides a mechanism to register callbacks to be called before and/or after insert, update and delete of records.

Each table stores six lists of callbacks:

``
db.mytable._before_insert
db.mytable._after_insert
db.mytable._before_update
db.mytable._after_update
db.mytable._before_delete
db.mytable._after_delete
``:code

You can register callback function by appending it the corresponding function to one of those lists. The caveat is that depending on the functionality, the callback has different signature.

This is best explained via some examples.

``
>>> db.define_table('person', Field('name'))
>>> def pprint(*args): print args
>>> db.person._before_insert.append(lambda f: pprint(f))
>>> db.person._after_insert.append(lambda f, id: pprint(f, id))
>>> db.person._before_update.append(lambda s, f: pprint(s, f))
>>> db.person._after_update.append(lambda s, f: pprint(s, f))
>>> db.person._before_delete.append(lambda s: pprint(s))
>>> db.person._after_delete.append(lambda s: pprint(s))
``:code

Here ``f`` is a dict of fields passed to insert or update, ``id`` is the id of the newly inserted record, ``s`` is the Set object used for update or delete.

``
>>> db.person.insert(name='John')
({'name': 'John'},)
({'name': 'John'}, 1)
>>> db(db.person.id==1).update(name='Tim')
(<Set (person.id = 1)>, {'name': 'Tim'})
(<Set (person.id = 1)>, {'name': 'Tim'})
>>> db(db.person.id==1).delete()
(<Set (person.id = 1)>,)
(<Set (person.id = 1)>,)
``:code

The return values of these callback should be ``None`` or ``False``. If any of the ``_before_*`` callback returns a ``True`` value it will abort the actual insert/update/delete operation.

``update_naive``:inxx

Some times a callback may need to perform an update in the same or a different table and one wants to avoid callbacks calling themselves recursively.

For this purpose there the Set objects have an ``update_naive`` method that works like ``update`` but ignores before and after callbacks.

##### Database cascades
Database schema can define relationships which trigger deletions of related records, known as cascading. The DAL is not informed when a record is deleted due to a cascade. So an ``on_delete`` trigger will not be called due a cascade-deletion. 

[[versioning]]
#### Record versioning
``_enable_record_versioning``:inxx

It is possible to ask web2py to save every copy of a record when the record is individually modified. There are different ways to do it and it can be done for all tables at once using the syntax:

``
auth.enable_record_versioning(db)
``:code

this requires Auth and it is discussed in the chapter about authentication.
It can also be done for each individual table as discussed below.

Consider the following table:

``
db.define_table('stored_item',
                Field('name'),
                Field('quantity', 'integer'),
                Field('is_active', 'boolean',
                      writable=False, readable=False, default=True))
``:code

Notice the hidden boolean field called ``is_active`` and defaulting to
True.

We can tell web2py to create a new table (in the same or a different database) and store all previous versions of each record in the table, when modified.

This is done in the following way:
``
db.stored_item._enable_record_versioning()
``:code

or in a more verbose syntax:

``
db.stored_item._enable_record_versioning(archive_db=db,
                                         archive_name='stored_item_archive',
                                         current_record='current_record',
                                         is_active='is_active')
``

The ``archive_db=db`` tells web2py to store the archive table in the same database as the ``stored_item`` table. The ``archive_name`` sets the name for the archive table. The archive table has the same fields as the original table ``stored_item`` except that unique fields are no longer unique (because it needs to store multiple versions) and has an extra field which name is specified by ``current_record`` and which is a reference to the current record in the ``stored_item`` table.

When records are deleted, they are not really deleted. A deleted record is copied in the ``stored_item_archive`` table (like when it is modified) and the ``is_active`` field is set to False. By enabling record versioning web2py sets a ``custom_filter`` on this table that hides all records in table ``stored_item`` where the ``is_active`` field is set to False. The ``is_active`` parameter in the ``_enable_record_versioning`` method allows to specify the name of the field used by the ``custom_filter`` to determine if the field was deleted or not.

``custom_filter``s are ignored by the appadmin interface.

#### Common fields and multi-tenancy
``common fields``:inxx
``multi tenancy``:inxx

``db._common_fields`` is a list of fields that should belong to all the tables. This list can also contain tables and it is understood as all fields from the table. For example occasionally you find yourself in need to add a signature to all your tables but the ```auth`` tables. In this case, after you ``db.define_tables()`` but before defining any other table, insert

``
db._common_fields.append(auth.signature)
``

One field is special: "request_tenant".
This field does not exist but you can create it and add it to any of your tables (or all of them):

``
db._common_fields.append(Field('request_tenant',
                               default=request.env.http_host,
                               writable=False))
``

For every table with a field called ``db._request_tenant``, all records for all queries are always automatically filtered by:

``
db.table.request_tenant == db.table.request_tenant.default
``:code

and for every record inserted, this field is set to the default value.
In the example above we have chosen
``
default = request.env.http_host
``
i.e. we have chose to ask our app to filter all tables in all queries with
``
db.table.request_tenant == request.env.http_host
``

This simple trick allow us to turn any application into a multi-tenant application. i.e. even if we run one instance of the app and we use one single database, if the app is accessed under two or more domains (in the example the domain name is retrieved from ``request.env.http_host``) the visitors will see different data depending on the domain. Think of running multiple web stores under different domains with one app and one database.

You can turn off multi tenancy filters using: ``ignore_common_filters``:inxx
``
rows = db(query, ignore_common_filters=True).select()
``:code

#### Common filters

A common filter is a generalization of the above multi-tenancy idea.
It provides an easy way to prevent repeating of the same query.
Consider for example the following table:

``
db.define_table('blog_post',
                Field('subject'),
                Field('post_text', 'text'),
                Field('is_public', 'boolean'),
                common_filter = lambda query: db.blog_post.is_public==True)
``

Any select, delete or update in this table, will include only public blog posts. The attribute can also be changed in controllers:

``
db.blog_post._common_filter = lambda query: db.blog_post.is_public == True
``

It serves both as a way to avoid repeating the "db.blog_post.is_public==True" phrase in each blog post search, and also as a security enhancement, that prevents you from forgetting to disallow viewing of non-public posts.

In case you actually do want items left out by the common filter (for example, allowing the admin to see non-public posts), you can either remove the filter:
``
db.blog_post._common_filter = None
``
or ignore it:
``
db(query, ignore_common_filters=True).select(...)
``

#### Custom ``Field`` types (experimental)

``SQLCustomType``:inxx

Aside for using ``filter_in`` and ``filter_out``, it is possible to define new/custom field types.
For example we consider here a field that contains binary data in compressed form:

``
from gluon.dal import SQLCustomType
import zlib

compressed = SQLCustomType(type ='text',
                           native='text',
                           encoder=(lambda x: zlib.compress(x or '')),
                           decoder=(lambda x: zlib.decompress(x)))

db.define_table('example', Field('data', type=compressed))
``:code

``SQLCustomType`` is a field type factory. Its ``type`` argument must be one of the standard web2py types. It tells web2py how to treat the field values at the web2py level. ``native`` is the type of the field as far as the database is concerned. Allowed names depend on the database engine. ``encoder`` is an optional transformation function applied when the data is stored and ``decoder`` is the optional reversed transformation function.

This feature is marked as experimental. In practice it has been in web2py for a long time and it works but it can make the code not portable, for example when the native type is database specific. It does not work on Google App Engine NoSQL.

#### Using DAL without define tables

The DAL can be used from any Python program simply by doing this:

``
from gluon import DAL, Field
db = DAL('sqlite://storage.sqlite', folder='path/to/app/databases')
``:code

i.e. import the DAL, Field, connect and specify the folder which contains the .table files (the app/databases folder).

To access the data and its attributes we still have to define all the tables we are going to access with ``db.define_tables(...)``.

If we just need access to the data but not to the web2py table attributes, we get away without re-defining the tables but simply asking web2py to read the necessary info from the metadata in the .table files:

``
from gluon import DAL, Field
db = DAL('sqlite://storage.sqlite', folder='path/to/app/databases', auto_import=True))
``:code

This allows us to access any ``db.table`` without need to re-define it.

#### PostGIS, SpatiaLite, and MS Geo (experimental)

``PostGIS``:inxx ``StatiaLite``:inxx ``Geo Extensions``:inxx
``geometry``:inxx ``geoPoint``:inxx ``geoLine``:inxx ``geoPolygon``:inxx

The DAL supports geographical APIs using PostGIS (for PostgreSQL), spatialite (for SQLite), and MSSQL and Spatial Extensions. This is a feature that was sponsored by the Sahana project and implemented by Denes Lengyel.

DAL provides geometry and geography fields types and the following functions:

``st_asgeojson``:inxx ``st_astext``:inxx ``st_contains``:inxx
``st_distance``:inxx ``st_equals``:inxx ``st_intersects``:inxx ``st_overlaps``:inxx
``st_simplify``:inxx ``st_touches``:inxx ``st_within``:inxx

``
st_asgeojson (PostGIS only)
st_astext
st_contains
st_distance
st_equals
st_intersects
st_overlaps
st_simplify (PostGIS only)
st_touches
st_within
st_x
st_y
``

Here are some examples:

``
from gluon.dal import DAL, Field, geoPoint, geoLine, geoPolygon
db = DAL("mssql://user:pass@host:db")
sp = db.define_table('spatial', Field('loc', 'geometry()'))
``:code

Below we insert a point, a line, and a polygon:
``
sp.insert(loc=geoPoint(1, 1))
sp.insert(loc=geoLine((100, 100), (20, 180), (180, 180)))
sp.insert(loc=geoPolygon((0, 0), (150, 0), (150, 150), (0, 150), (0, 0)))
``:code

Notice that
``
rows = db(sp.id > 0).select()
``:code

Always returns the geometry data serialized as text.
You can also do the same more explicitly using ``st_astext()``:

``
print db(sp.id>0).select(sp.id, sp.loc.st_astext())
spatial.id,spatial.loc.STAsText()
1, "POINT (1 2)"
2, "LINESTRING (100 100, 20 180, 180 180)"
3, "POLYGON ((0 0, 150 0, 150 150, 0 150, 0 0))"
``:code

You can ask for the native representation by using ``st_asgeojson()`` (in PostGIS only):

``
print db(sp.id>0).select(sp.id, sp.loc.st_asgeojson().with_alias('loc'))
spatial.id,loc
1, [1, 2]
2, [[100, 100], [20 180], [180, 180]]
3, [[[0, 0], [150, 0], [150, 150], [0, 150], [0, 0]]]
``:code

(notice an array is a point, an array of arrays is a line, and an array of array of arrays is a polygon).

Here are example of how to use geographical functions:

``
query = sp.loc.st_intersects(geoLine((20, 120), (60, 160)))
query = sp.loc.st_overlaps(geoPolygon((1, 1), (11, 1), (11, 11), (11, 1), (1, 1)))
query = sp.loc.st_contains(geoPoint(1, 1))
print db(query).select(sp.id, sp.loc)
spatial.id, spatial.loc
3,"POLYGON ((0 0, 150 0, 150 150, 0 150, 0 0))"
``:code

Computed distances can also be retrieved as floating point numbers:

``
dist = sp.loc.st_distance(geoPoint(-1,2)).with_alias('dist')
print db(sp.id>0).select(sp.id, dist)
spatial.id, dist
1 2.0
2 140.714249456
3 1.0
``:code

#### Copy data from one db into another

Consider the situation in which you have been using the following database:

``
db = DAL('sqlite://storage.sqlite')
``

and you wish to move to another database using a different connection string:

``
db = DAL('postgres://username:password@localhost/mydb')
``

Before you switch, you want to move the data and rebuild all the metadata for the new database. We assume the new database to exist but we also assume it is empty.

Web2py provides a script that does this work for you:

``
cd web2py
python scripts/cpdb.py \\
   -f applications/app/databases \\
   -y 'sqlite://storage.sqlite' \\
   -Y 'postgres://username:password@localhost/mydb' \\
   -d ../gluon
``

After running the script you can simply switch the connection string in the model and everything should work out of the box. The new data should be there.

This script provides various command line options that allows you to move data from one application to another, move all tables or only some tables, clear the data in the tables. For more info try:

``
python scripts/cpdb.py -h
``

#### Note on new DAL and adapters

The source code of the Database Abstraction Layer was completely rewritten in 2010. While it stays backward compatible, the rewrite made it more modular and easier to extend. Here we explain the main logic.

The file "gluon/dal.py" defines, among other, the following classes.

``
ConnectionPool
BaseAdapter extends ConnectionPool
Row
DAL
Reference
Table
Expression
Field
Query
Set
Rows
``

Their use has been explained in the previous sections, except for ``BaseAdapter``. When the methods of a ``Table`` or ``Set`` object need to communicate with the database they delegate to methods of the adapter the task to generate the SQL and or the function call.

For example:

``
db.mytable.insert(myfield='myvalue')
``

calls

``
Table.insert(myfield='myvalue')
``

which delegates the adapter by returning:

``
db._adapter.insert(db.mytable, db.mytable._listify(dict(myfield='myvalue')))
``

Here ``db.mytable._listify`` converts the dict of arguments into a list of ``(field,value)`` and calls the ``insert`` method of the ``adapter``. ``db._adapter`` does more or less the following:

``
query = db._adapter._insert(db.mytable, list_of_fields)
db._adapter.execute(query)
``

where the first line builds the query and the second executes it.

``BaseAdapter`` defines the interface for all adapters.

"gluon/dal.py" at the moment of writing this book, contains the following adapters:

``
SQLiteAdapter extends BaseAdapter
JDBCSQLiteAdapter extends SQLiteAdapter
MySQLAdapter extends BaseAdapter
PostgreSQLAdapter extends BaseAdapter
JDBCPostgreSQLAdapter extends PostgreSQLAdapter
OracleAdapter extends BaseAdapter
MSSQLAdapter extends BaseAdapter
MSSQL2Adapter extends MSSQLAdapter
MSSQL3Adapter extends MSSQLAdapter
MSSQL4Adapter extends MSSQLAdapter
FireBirdAdapter extends BaseAdapter
FireBirdEmbeddedAdapter extends FireBirdAdapter
InformixAdapter extends BaseAdapter
DB2Adapter extends BaseAdapter
IngresAdapter extends BaseAdapter
IngresUnicodeAdapter extends IngresAdapter
GoogleSQLAdapter extends MySQLAdapter
NoSQLAdapter extends BaseAdapter
GoogleDatastoreAdapter extends NoSQLAdapter
CubridAdapter extends MySQLAdapter (experimental)
TeradataAdapter extends DB2Adapter (experimental)
SAPDBAdapter extends BaseAdapter (experimental)
CouchDBAdapter extends NoSQLAdapter (experimental)
IMAPAdapter extends NoSQLAdapter (experimental)
MongoDBAdapter extends NoSQLAdapter (experimental)
VerticaAdapter extends MSSQLAdapter (experimental)
SybaseAdapter extends MSSQLAdapter (experimental)
``

which override the behavior of the ``BaseAdapter``.

Each adapter has more or less this structure:

``
class MySQLAdapter(BaseAdapter):

    # specify a diver to use
    driver = globals().get('pymysql', None)

    # map web2py types into database types
    types = {
        'boolean': 'CHAR(1)',
        'string': 'VARCHAR(%(length)s)',
        'text': 'LONGTEXT',
        ...
        }

    # connect to the database using driver
    def __init__(self, db, uri, pool_size=0, folder=None, db_codec ='UTF-8',
                 credential_decoder=lambda x:x, driver_args={},
                 adapter_args={}):
        # parse uri string and store parameters in driver_args
        ...
        # define a connection function
        def connect(driver_args=driver_args):
            return self.driver.connect(**driver_args)
        # place it in the pool
        self.pool_connection(connect)
        # set optional parameters (after connection)
        self.execute('SET FOREIGN_KEY_CHECKS=1;')
        self.execute("SET sql_mode='NO_BACKSLASH_ESCAPES';")

   # override BaseAdapter methods as needed
   def lastrowid(self, table):
        self.execute('select last_insert_id();')
        return int(self.cursor.fetchone()[0])

``:code

Looking at the various adapters as example should be easy to write new ones.

When ``db`` instance is created:

``
db = DAL('mysql://...')
``

the prefix in the uri string defines the adapter. The mapping is defined in the following dictionary also in "gluon/dal.py":

``
ADAPTERS = {
    'sqlite': SQLiteAdapter,
    'spatialite': SpatiaLiteAdapter,
    'sqlite:memory': SQLiteAdapter,
    'spatialite:memory': SpatiaLiteAdapter,
    'mysql': MySQLAdapter,
    'postgres': PostgreSQLAdapter,
    'postgres:psycopg2': PostgreSQLAdapter,
    'postgres2:psycopg2': NewPostgreSQLAdapter,
    'oracle': OracleAdapter,
    'mssql': MSSQLAdapter,
    'mssql2': MSSQL2Adapter,
    'mssql3': MSSQL3Adapter,
    'mssql4' : MSSQL4Adapter,
    'vertica': VerticaAdapter,
    'sybase': SybaseAdapter,
    'db2': DB2Adapter,
    'teradata': TeradataAdapter,
    'informix': InformixAdapter,
    'informix-se': InformixSEAdapter,
    'firebird': FireBirdAdapter,
    'firebird_embedded': FireBirdAdapter,
    'ingres': IngresAdapter,
    'ingresu': IngresUnicodeAdapter,
    'sapdb': SAPDBAdapter,
    'cubrid': CubridAdapter,
    'jdbc:sqlite': JDBCSQLiteAdapter,
    'jdbc:sqlite:memory': JDBCSQLiteAdapter,
    'jdbc:postgres': JDBCPostgreSQLAdapter,
    'gae': GoogleDatastoreAdapter, # discouraged, for backward compatibility
    'google:datastore': GoogleDatastoreAdapter,
    'google:datastore+ndb': GoogleDatastoreAdapter,
    'google:sql': GoogleSQLAdapter,
    'couchdb': CouchDBAdapter,
    'mongodb': MongoDBAdapter,
    'imap': IMAPAdapter
}
``:code

the uri string is then parsed in more detail by the adapter itself.

For any adapter you can replace the driver with a different one:

``
import MySQLdb as mysqldb
from gluon.dal import MySQLAdapter
MySQLAdapter.driver = mysqldb
``
i.e. ``mysqldb`` has to be ''that module'' with a .connect() method.
You can specify optional driver arguments and adapter arguments:

``
db =DAL(..., driver_args={}, adapter_args={})
``


### Gotchas

#### SQLite
SQLite does not support dropping and altering columns. That means that web2py migrations will work up to a point. If you delete a field from a table, the column will remain in the database but will be invisible to web2py. If you decide to reinstate the column, web2py will try re-create it and fail. In this case you must set ``fake_migrate=True`` so that metadata is rebuilt without attempting to add the column again. Also, for the same reason, **SQLite** is not aware of any change of column type. If you insert a number in a string field, it will be stored as string. If you later change the model and replace the type "string" with type "integer", SQLite will continue to keep the number as a string and this may cause problem when you try to extract the data.

SQLite doesn't have a boolean type. web2py internally maps booleans to a 1 character string, with 'T' and 'F' representing True and False. The DAL handles this completely; the abstraction of a true boolean value works well.
But if you are updating the SQLite table with SQL directly, be aware of the web2py implementation, and avoid using 0 and 1 values.

#### MySQL

MySQL does not support multiple ALTER TABLE within a single transaction. This means that any migration process is broken into multiple commits. If something happens that causes a failure it is possible to break a migration (the web2py metadata are no longer in sync with the actual table structure in the database). This is unfortunate but it can be prevented (migrate one table at the time) or it can be fixed a posteriori (revert the web2py model to what corresponds to the table structure in database, set ``fake_migrate=True`` and after the metadata has been rebuilt, set ``fake_migrate=False`` and migrate the table again).

#### Google SQL

Google SQL has the same problems as MySQL and more. In particular table metadata itself must be stored in the database in a table that is not migrated by web2py. This is because Google App Engine has a read-only file system. Web2py migrations in Google:SQL combined with the MySQL issue described above can result in metadata corruption. Again, this can be prevented (by migrating the table at once and then setting migrate=False so that the metadata table is not accessed any more) or it can fixed a posteriori (by accessing the database using the Google dashboard and deleting any corrupted entry from the table called ``web2py_filesystem``.

#### MSSQL (Microsoft SQL Server)
``limitby``:inxx
MSSQL < 2012 does not support the SQL OFFSET keyword. Therefore the database cannot do pagination. When doing a ``limitby=(a, b)`` web2py will fetch the first ``b`` rows and discard the first ``a``. This may result in a considerable overhead when compared with other database engines.
If you're using MSSQL >= 2005, the recommended prefix to use is ``mssql3://`` which provides a method to avoid the issue of fetching the entire non-paginated resultset. If you're on MSSQL >= 2012, use ``mssql4://`` that uses the ``OFFSET ... ROWS ... FETCH NEXT ... ROWS ONLY`` construct to support natively pagination without performance hits like other backends.
The ``mssql://`` uri also enforces (for historical reasons) the use of ``text`` columns, that are superseeded in more recent versions (from 2005 onwards) by ``varchar(max)``. ``mssql3://`` and ``mssql4://`` should be used if you don't want to face some limitations of the - officially deprecated - ``text`` columns

MSSQL has problems with circular references in tables that have ONDELETE CASCADE. This is an MSSQL bug and you work around it by setting the ondelete attribute for all reference fields to "NO ACTION". 
You can also do it once and for all before you define tables:

``
db = DAL('mssql://....')
for key in ['reference', 'reference FK']:
    db._adapter.types[key]=db._adapter.types[key].replace('%(on_delete_action)s', 'NO ACTION')
``:code

MSSQL also has problems with arguments passed to the DISTINCT keyword and therefore
 while this works,

``
db(query).select(distinct=True)
``

this does not

``
db(query).select(distinct=db.mytable.myfield)
``

#### Oracle

Oracle also does not support pagination. It does not support neither the OFFSET nor the LIMIT keywords. Web2py achieves pagination by translating a ``db(...).select(limitby=(a, b))`` into a complex three-way nested select (as suggested by official Oracle documentation).
This works for simple select but may break for complex selects involving aliased fields and or joins.

#### Google NoSQL (Datastore)
Google NoSQL (Datastore) does not allow joins, left joins, aggregates, expression, OR involving more than one table, the ‘like’ operator searches in "text" fields. 

Transactions are limited and not provided automatically by web2py (you need to use the Google API ``run_in_transaction`` which you can look up in the Google App Engine documentation online). 

Google also limits the number of records you can retrieve in each one query (1000 at the time of writing). On the Google datastore record IDs are integer but they are not sequential. 
While on SQL the "list:string" type is mapped into a "text" type, on the Google Datastore it is mapped into a ``ListStringProperty``. Similarly "list:integer" and "list:reference" are mapped into "ListProperty". This makes searches for content inside these fields types are more efficient on Google NoSQL than on SQL databases.
